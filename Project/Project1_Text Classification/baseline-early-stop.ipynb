{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import requirements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-10-24T21:28:01.108943Z",
     "iopub.status.busy": "2021-10-24T21:28:01.108662Z",
     "iopub.status.idle": "2021-10-24T21:28:07.275681Z",
     "shell.execute_reply": "2021-10-24T21:28:07.274712Z",
     "shell.execute_reply.started": "2021-10-24T21:28:01.108865Z"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import pdb\n",
    "import wandb\n",
    "import argparse\n",
    "from dataclasses import dataclass, field\n",
    "from typing import Optional\n",
    "from collections import defaultdict\n",
    "\n",
    "import torch\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from tqdm import tqdm, trange\n",
    "\n",
    "from torchvision import datasets\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data.sampler import SubsetRandomSampler\n",
    "\n",
    "from transformers import (\n",
    "    BertForSequenceClassification,\n",
    "    BertTokenizer,\n",
    "    AutoConfig,\n",
    "    AdamW\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mohikendoit\u001b[0m (use `wandb login --relogin` to force relogin)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: wandb version 0.12.5 is available!  To upgrade, please run:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  $ pip install wandb --upgrade\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                Tracking run with wandb version 0.10.31<br/>\n",
       "                Syncing run <strong style=\"color:#cdcd00\">devoted-blaze-7</strong> to <a href=\"https://wandb.ai\" target=\"_blank\">Weights & Biases</a> <a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">(Documentation)</a>.<br/>\n",
       "                Project page: <a href=\"https://wandb.ai/ohikendoit/goorm_text_classification\" target=\"_blank\">https://wandb.ai/ohikendoit/goorm_text_classification</a><br/>\n",
       "                Run page: <a href=\"https://wandb.ai/ohikendoit/goorm_text_classification/runs/2cihx2xt\" target=\"_blank\">https://wandb.ai/ohikendoit/goorm_text_classification/runs/2cihx2xt</a><br/>\n",
       "                Run data is saved locally in <code>/home/ken/PycharmProjects/goorm_kaist_nlp/Project/Project1_Text Classification/wandb/run-20211028_062611-2cihx2xt</code><br/><br/>\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<h1>Run(2cihx2xt)</h1><iframe src=\"https://wandb.ai/ohikendoit/goorm_text_classification/runs/2cihx2xt\" style=\"border:none;width:100%;height:400px\"></iframe>"
      ],
      "text/plain": [
       "<wandb.sdk.wandb_run.Run at 0x7f6963c5c730>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wandb.init(\n",
    "    project='goorm_text_classification',\n",
    "    entity='ohikendoit'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 0. Early Stopping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EarlyStopping:\n",
    "    \"\"\"주어진 patience 이후로 validation loss가 개선되지 않으면 학습을 조기 중지\"\"\"\n",
    "    def __init__(self, patience=7, verbose=False, delta=0, path='checkpoint.pt'):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            patience (int): validation loss가 개선된 후 기다리는 기간\n",
    "                            Default: 7\n",
    "            verbose (bool): True일 경우 각 validation loss의 개선 사항 메세지 출력\n",
    "                            Default: False\n",
    "            delta (float): 개선되었다고 인정되는 monitered quantity의 최소 변화\n",
    "                            Default: 0\n",
    "            path (str): checkpoint저장 경로\n",
    "                            Default: 'checkpoint.pt'\n",
    "        \"\"\"\n",
    "        self.patience = patience\n",
    "        self.verbose = verbose\n",
    "        self.counter = 0\n",
    "        self.best_score = None\n",
    "        self.early_stop = False\n",
    "        self.val_loss_min = np.Inf\n",
    "        self.delta = delta\n",
    "        self.path = path\n",
    "\n",
    "    def __call__(self, val_loss, model):\n",
    "\n",
    "        score = -val_loss\n",
    "\n",
    "        if self.best_score is None:\n",
    "            self.best_score = score\n",
    "            self.save_checkpoint(val_loss, model)\n",
    "        elif score < self.best_score + self.delta:\n",
    "            self.counter += 1\n",
    "            print(f'EarlyStopping counter: {self.counter} out of {self.patience}')\n",
    "            if self.counter >= self.patience:\n",
    "                self.early_stop = True\n",
    "        else:\n",
    "            self.best_score = score\n",
    "            self.save_checkpoint(val_loss, model)\n",
    "            self.counter = 0\n",
    "\n",
    "    def save_checkpoint(self, val_loss, model):\n",
    "        '''validation loss가 감소하면 모델을 저장한다.'''\n",
    "        if self.verbose:\n",
    "            print(f'Validation loss decreased ({self.val_loss_min:.6f} --> {val_loss:.6f}).  Saving model ...')\n",
    "        torch.save(model.state_dict(), self.path)\n",
    "        self.val_loss_min = val_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Preprocess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-10-24T21:05:14.091686Z",
     "iopub.status.busy": "2021-10-24T21:05:14.091032Z",
     "iopub.status.idle": "2021-10-24T21:05:14.098044Z",
     "shell.execute_reply": "2021-10-24T21:05:14.097299Z",
     "shell.execute_reply.started": "2021-10-24T21:05:14.091649Z"
    }
   },
   "outputs": [],
   "source": [
    "def make_id_file(task, tokenizer):\n",
    "    def make_data_strings(file_name):\n",
    "        data_strings = []\n",
    "        with open(os.path.join('/home/ken/PycharmProjects/goorm_kaist_nlp/Project/Project1_Text Classification/data', file_name), 'r', encoding='utf-8') as f:\n",
    "            id_file_data = [tokenizer.encode(line.lower()) for line in f.readlines()]\n",
    "        for item in id_file_data:\n",
    "            data_strings.append(' '.join([str(k) for k in item]))\n",
    "        return data_strings\n",
    "    \n",
    "    print('it will take some times...')\n",
    "    train_pos = make_data_strings('sentiment.train.1')\n",
    "    train_neg = make_data_strings('sentiment.train.0')\n",
    "    dev_pos = make_data_strings('sentiment.dev.1')\n",
    "    dev_neg = make_data_strings('sentiment.dev.0')\n",
    "\n",
    "    print('make id file finished!')\n",
    "    return train_pos, train_neg, dev_pos, dev_neg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-10-24T21:05:15.841446Z",
     "iopub.status.busy": "2021-10-24T21:05:15.840891Z",
     "iopub.status.idle": "2021-10-24T21:05:20.5036Z",
     "shell.execute_reply": "2021-10-24T21:05:20.502899Z",
     "shell.execute_reply.started": "2021-10-24T21:05:15.841408Z"
    }
   },
   "outputs": [],
   "source": [
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-10-24T21:05:20.505189Z",
     "iopub.status.busy": "2021-10-24T21:05:20.504949Z",
     "iopub.status.idle": "2021-10-24T21:07:57.034951Z",
     "shell.execute_reply": "2021-10-24T21:07:57.033206Z",
     "shell.execute_reply.started": "2021-10-24T21:05:20.505155Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "it will take some times...\n",
      "make id file finished!\n"
     ]
    }
   ],
   "source": [
    "train_pos, train_neg, dev_pos, dev_neg = make_id_file('yelp', tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-10-24T21:08:00.592861Z",
     "iopub.status.busy": "2021-10-24T21:08:00.592452Z",
     "iopub.status.idle": "2021-10-24T21:08:00.601702Z",
     "shell.execute_reply": "2021-10-24T21:08:00.600822Z",
     "shell.execute_reply.started": "2021-10-24T21:08:00.592824Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['101 6581 2833 1012 102',\n",
       " '101 21688 8013 2326 1012 102',\n",
       " '101 2027 2036 2031 3679 19247 1998 3256 6949 2029 2003 2428 2204 1012 102',\n",
       " '101 2009 1005 1055 1037 2204 15174 2098 7570 22974 2063 1012 102',\n",
       " '101 1996 3095 2003 5379 1012 102',\n",
       " '101 2204 3347 2833 1012 102',\n",
       " '101 2204 2326 1012 102',\n",
       " '101 11350 1997 2154 2003 25628 1998 7167 1997 19247 1012 102',\n",
       " '101 2307 2173 2005 6265 2030 3347 27962 1998 5404 1012 102',\n",
       " '101 1996 2047 2846 3504 6429 1012 102']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_pos[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-10-24T21:08:01.251292Z",
     "iopub.status.busy": "2021-10-24T21:08:01.250691Z",
     "iopub.status.idle": "2021-10-24T21:08:01.260285Z",
     "shell.execute_reply": "2021-10-24T21:08:01.259365Z",
     "shell.execute_reply.started": "2021-10-24T21:08:01.251247Z"
    }
   },
   "outputs": [],
   "source": [
    "class SentimentDataset(object):\n",
    "    def __init__(self, tokenizer, pos, neg):\n",
    "        self.tokenizer = tokenizer\n",
    "        self.data = []\n",
    "        self.label = []\n",
    "\n",
    "        for pos_sent in pos:\n",
    "            self.data += [self._cast_to_int(pos_sent.strip().split())]\n",
    "            self.label += [[1]]\n",
    "        for neg_sent in neg:\n",
    "            self.data += [self._cast_to_int(neg_sent.strip().split())]\n",
    "            self.label += [[0]]\n",
    "        self.data = self.data[:1000]\n",
    "\n",
    "    def _cast_to_int(self, sample):\n",
    "        return [int(word_id) for word_id in sample]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        sample = self.data[index]\n",
    "        return np.array(sample), np.array(self.label[index])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-10-24T21:08:01.994746Z",
     "iopub.status.busy": "2021-10-24T21:08:01.99441Z",
     "iopub.status.idle": "2021-10-24T21:08:04.915342Z",
     "shell.execute_reply": "2021-10-24T21:08:04.914594Z",
     "shell.execute_reply.started": "2021-10-24T21:08:01.994715Z"
    }
   },
   "outputs": [],
   "source": [
    "train_dataset = SentimentDataset(tokenizer, train_pos, train_neg)\n",
    "dev_dataset = SentimentDataset(tokenizer, dev_pos, dev_neg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-10-24T21:08:06.066445Z",
     "iopub.status.busy": "2021-10-24T21:08:06.065969Z",
     "iopub.status.idle": "2021-10-24T21:08:06.07667Z",
     "shell.execute_reply": "2021-10-24T21:08:06.075849Z",
     "shell.execute_reply.started": "2021-10-24T21:08:06.066411Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(array([ 101, 6581, 2833, 1012,  102]), array([1]))\n",
      "(array([  101, 21688,  8013,  2326,  1012,   102]), array([1]))\n",
      "(array([  101,  2027,  2036,  2031,  3679, 19247,  1998,  3256,  6949,\n",
      "        2029,  2003,  2428,  2204,  1012,   102]), array([1]))\n",
      "(array([  101,  2009,  1005,  1055,  1037,  2204, 15174,  2098,  7570,\n",
      "       22974,  2063,  1012,   102]), array([1]))\n",
      "(array([ 101, 1996, 3095, 2003, 5379, 1012,  102]), array([1]))\n",
      "(array([ 101, 2204, 3347, 2833, 1012,  102]), array([1]))\n",
      "(array([ 101, 2204, 2326, 1012,  102]), array([1]))\n",
      "(array([  101, 11350,  1997,  2154,  2003, 25628,  1998,  7167,  1997,\n",
      "       19247,  1012,   102]), array([1]))\n",
      "(array([  101,  2307,  2173,  2005,  6265,  2030,  3347, 27962,  1998,\n",
      "        5404,  1012,   102]), array([1]))\n",
      "(array([ 101, 1996, 2047, 2846, 3504, 6429, 1012,  102]), array([1]))\n",
      "(array([ 101, 2023, 2173, 2001, 2200, 2204, 1012,  102]), array([1]))\n"
     ]
    }
   ],
   "source": [
    "for i, item in enumerate(train_dataset):\n",
    "    print(item)\n",
    "    if i == 10:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-10-24T21:08:06.674726Z",
     "iopub.status.busy": "2021-10-24T21:08:06.674262Z",
     "iopub.status.idle": "2021-10-24T21:08:06.682396Z",
     "shell.execute_reply": "2021-10-24T21:08:06.68173Z",
     "shell.execute_reply.started": "2021-10-24T21:08:06.674692Z"
    }
   },
   "outputs": [],
   "source": [
    "def collate_fn_style(samples):\n",
    "    input_ids, labels = zip(*samples)\n",
    "    max_len = max(len(input_id) for input_id in input_ids)\n",
    "    sorted_indices = np.argsort([len(input_id) for input_id in input_ids])[::-1]\n",
    "\n",
    "    input_ids = pad_sequence([torch.tensor(input_ids[index]) for index in sorted_indices],\n",
    "                             batch_first=True)\n",
    "    attention_mask = torch.tensor(\n",
    "        [[1] * len(input_ids[index]) + [0] * (max_len - len(input_ids[index])) for index in\n",
    "         sorted_indices])\n",
    "    token_type_ids = torch.tensor([[0] * len(input_ids[index]) for index in sorted_indices])\n",
    "    position_ids = torch.tensor([list(range(len(input_ids[index]))) for index in sorted_indices])\n",
    "    labels = torch.tensor(np.stack(labels, axis=0)[sorted_indices])\n",
    "\n",
    "    return input_ids, attention_mask, token_type_ids, position_ids, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-10-24T21:08:07.207743Z",
     "iopub.status.busy": "2021-10-24T21:08:07.20726Z",
     "iopub.status.idle": "2021-10-24T21:08:07.214411Z",
     "shell.execute_reply": "2021-10-24T21:08:07.213596Z",
     "shell.execute_reply.started": "2021-10-24T21:08:07.207708Z"
    }
   },
   "outputs": [],
   "source": [
    "train_batch_size=64\n",
    "eval_batch_size=64\n",
    "\n",
    "def create_datasets(batch_size):\n",
    "    # trainning set 중 validation 데이터로 사용할 비율\n",
    "    valid_size = 0.2\n",
    "\n",
    "    # torch.FloatTensor로 변환\n",
    "    transform = transforms.ToTensor()\n",
    "\n",
    "    # choose the training and test datasets\n",
    "    train_data = SentimentDataset(tokenizer, train_pos, train_neg)\n",
    "    test_data = SentimentDataset(tokenizer, dev_pos, dev_neg)\n",
    "\n",
    "    # validation으로 사용할 trainning indices를 얻는다.\n",
    "    num_train = len(train_data)\n",
    "    indices = list(range(num_train))\n",
    "    np.random.shuffle(indices)\n",
    "    split = int(np.floor(valid_size * num_train))\n",
    "    train_idx, valid_idx = indices[split:], indices[:split]\n",
    "\n",
    "    # trainning, validation batch를 얻기 위한 sampler정의\n",
    "    train_sampler = SubsetRandomSampler(train_idx)\n",
    "    valid_sampler = SubsetRandomSampler(valid_idx)\n",
    "\n",
    "    # load training data in batches\n",
    "    train_loader = torch.utils.data.DataLoader(train_dataset,\n",
    "                                           batch_size=train_batch_size,\n",
    "                                           shuffle=True,\n",
    "                                           pin_memory=True, num_workers=4)\n",
    "\n",
    "    # load validation data in batches\n",
    "    valid_loader = torch.utils.data.DataLoader(train_dataset,\n",
    "                                           batch_size=train_batch_size,\n",
    "                                           shuffle=True,\n",
    "                                           pin_memory=True, num_workers=4)\n",
    "\n",
    "    # load test data in batches\n",
    "    test_loader = torch.utils.data.DataLoader(dev_dataset, batch_size=eval_batch_size,\n",
    "                                         shuffle=False,\n",
    "                                         num_workers=2)\n",
    "\n",
    "    return train_loader, test_loader, valid_loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "_kg_hide-input": true,
    "_kg_hide-output": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForSequenceClassification: ['cls.predictions.transform.dense.weight', 'cls.seq_relationship.bias', 'cls.predictions.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.bias']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "BertForSequenceClassification(\n",
       "  (bert): BertModel(\n",
       "    (embeddings): BertEmbeddings(\n",
       "      (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
       "      (position_embeddings): Embedding(512, 768)\n",
       "      (token_type_embeddings): Embedding(2, 768)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): BertEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (1): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (2): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (3): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (4): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (5): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (6): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (7): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (8): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (9): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (10): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (11): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (pooler): BertPooler(\n",
       "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "      (activation): Tanh()\n",
       "    )\n",
       "  )\n",
       "  (dropout): Dropout(p=0.1, inplace=False)\n",
       "  (classifier): Linear(in_features=768, out_features=2, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# random seed\n",
    "random_seed=42\n",
    "np.random.seed(random_seed)\n",
    "torch.manual_seed(random_seed)\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "model = BertForSequenceClassification.from_pretrained('bert-base-uncased')\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-10-24T21:08:14.848735Z",
     "iopub.status.busy": "2021-10-24T21:08:14.848317Z",
     "iopub.status.idle": "2021-10-24T21:08:14.858048Z",
     "shell.execute_reply": "2021-10-24T21:08:14.857342Z",
     "shell.execute_reply.started": "2021-10-24T21:08:14.848698Z"
    }
   },
   "outputs": [],
   "source": [
    "model.train()\n",
    "learning_rate = 5e-5\n",
    "optimizer = AdamW(model.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-10-24T21:08:15.790643Z",
     "iopub.status.busy": "2021-10-24T21:08:15.790143Z",
     "iopub.status.idle": "2021-10-24T21:08:15.795368Z",
     "shell.execute_reply": "2021-10-24T21:08:15.794439Z",
     "shell.execute_reply.started": "2021-10-24T21:08:15.790601Z"
    }
   },
   "outputs": [],
   "source": [
    "def compute_acc(predictions, target_labels):\n",
    "    return (np.array(predictions) == np.array(target_labels)).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, batch_size, patience, n_epochs):\n",
    "\n",
    "    # 모델이 학습되는 동안 trainning loss를 track\n",
    "    train_losses = []\n",
    "    # 모델이 학습되는 동안 validation loss를 track\n",
    "    valid_losses = []\n",
    "    # epoch당 average training loss를 track\n",
    "    avg_train_losses = []\n",
    "    # epoch당 average validation loss를 track\n",
    "    avg_valid_losses = []\n",
    "\n",
    "    # early_stopping object의 초기화\n",
    "    early_stopping = EarlyStopping(patience = patience, verbose = True)\n",
    "\n",
    "    for epoch in range(1, n_epochs + 1):\n",
    "\n",
    "        ###################\n",
    "        # train the model #\n",
    "        ###################\n",
    "        model.train() # prep model for training\n",
    "        for batch, (data, target) in enumerate(train_loader, 1):\n",
    "            # clear the gradients of all optimized variables\n",
    "            optimizer.zero_grad()    \n",
    "            # forward pass: 입력된 값을 모델로 전달하여 예측 출력 계산\n",
    "            output = model(data)\n",
    "            # calculate the loss\n",
    "            loss = criterion(output, target)\n",
    "            # backward pass: 모델의 파라미터와 관련된 loss의 그래디언트 계산\n",
    "            loss.backward()\n",
    "            # perform a single optimization step (parameter update)\n",
    "            optimizer.step()\n",
    "            # record training loss\n",
    "            train_losses.append(loss.item())\n",
    "\n",
    "\n",
    "        ######################    \n",
    "        # validate the model #\n",
    "        ######################\n",
    "        model.eval() # prep model for evaluation\n",
    "        for data , target in valid_loader :\n",
    "            # forward pass: 입력된 값을 모델로 전달하여 예측 출력 계산\n",
    "            output = model(data)\n",
    "            # calculate the loss\n",
    "            loss = criterion(output, target)\n",
    "            # record validation loss\n",
    "            valid_losses.append(loss.item())\n",
    "\n",
    "        # print 학습/검증 statistics\n",
    "        # epoch당 평균 loss 계산\n",
    "        train_loss = np.average(train_losses)\n",
    "        valid_loss = np.average(valid_losses)\n",
    "        avg_train_losses.append(train_loss)\n",
    "        avg_valid_losses.append(valid_loss)\n",
    "\n",
    "        epoch_len = len(str(n_epochs))\n",
    "\n",
    "\n",
    "        print_msg = (f'[{epoch:>{epoch_len}}/{n_epochs:>{epoch_len}}] ' +\n",
    "                     f'train_loss: {train_loss:.5f} ' +\n",
    "                     f'valid_loss: {valid_loss:.5f}')\n",
    "\n",
    "        print(print_msg)\n",
    "\n",
    "        # clear lists to track next epoch\n",
    "        train_losses = []\n",
    "        valid_losses = []\n",
    "\n",
    "        # early_stopping는 validation loss가 감소하였는지 확인이 필요하며,\n",
    "        # 만약 감소하였을경우 현제 모델을 checkpoint로 만든다.\n",
    "        early_stopping(valid_loss, model)\n",
    "\n",
    "        if early_stopping.early_stop:\n",
    "            print(\"Early stopping\")\n",
    "            break\n",
    "\n",
    "   # best model이 저장되어있는 last checkpoint를 로드한다.\n",
    "    model.load_state_dict(torch.load('checkpoint.pt'))\n",
    "\n",
    "    return  model, avg_train_losses, avg_valid_losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Caught RuntimeError in DataLoader worker process 0.\nOriginal Traceback (most recent call last):\n  File \"/home/ken/anaconda3/lib/python3.8/site-packages/torch/utils/data/_utils/worker.py\", line 287, in _worker_loop\n    data = fetcher.fetch(index)\n  File \"/home/ken/anaconda3/lib/python3.8/site-packages/torch/utils/data/_utils/fetch.py\", line 52, in fetch\n    return self.collate_fn(data)\n  File \"/home/ken/anaconda3/lib/python3.8/site-packages/torch/utils/data/_utils/collate.py\", line 84, in default_collate\n    return [default_collate(samples) for samples in transposed]\n  File \"/home/ken/anaconda3/lib/python3.8/site-packages/torch/utils/data/_utils/collate.py\", line 84, in <listcomp>\n    return [default_collate(samples) for samples in transposed]\n  File \"/home/ken/anaconda3/lib/python3.8/site-packages/torch/utils/data/_utils/collate.py\", line 64, in default_collate\n    return default_collate([torch.as_tensor(b) for b in batch])\n  File \"/home/ken/anaconda3/lib/python3.8/site-packages/torch/utils/data/_utils/collate.py\", line 56, in default_collate\n    return torch.stack(batch, 0, out=out)\nRuntimeError: stack expects each tensor to be equal size, but got [7] at entry 0 and [13] at entry 1\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-23-11d9ba8c167a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0mpatience\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m5\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalid_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpatience\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_epochs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-22-e00fccd046ba>\u001b[0m in \u001b[0;36mtrain_model\u001b[0;34m(model, batch_size, patience, n_epochs)\u001b[0m\n\u001b[1;32m     19\u001b[0m         \u001b[0;31m###################\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m         \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# prep model for training\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 21\u001b[0;31m         \u001b[0;32mfor\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     22\u001b[0m             \u001b[0;31m# clear the gradients of all optimized variables\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m             \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.8/site-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    519\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sampler_iter\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    520\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 521\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    522\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_num_yielded\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    523\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_kind\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0m_DatasetKind\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mIterable\u001b[0m \u001b[0;32mand\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.8/site-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1201\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1202\u001b[0m                 \u001b[0;32mdel\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_task_info\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1203\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_process_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1204\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1205\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_try_put_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.8/site-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_process_data\u001b[0;34m(self, data)\u001b[0m\n\u001b[1;32m   1227\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_try_put_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1228\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mExceptionWrapper\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1229\u001b[0;31m             \u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreraise\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1230\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1231\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.8/site-packages/torch/_utils.py\u001b[0m in \u001b[0;36mreraise\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    432\u001b[0m             \u001b[0;31m# instantiate since we don't know how to\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    433\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mRuntimeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmsg\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 434\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mexception\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    435\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    436\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Caught RuntimeError in DataLoader worker process 0.\nOriginal Traceback (most recent call last):\n  File \"/home/ken/anaconda3/lib/python3.8/site-packages/torch/utils/data/_utils/worker.py\", line 287, in _worker_loop\n    data = fetcher.fetch(index)\n  File \"/home/ken/anaconda3/lib/python3.8/site-packages/torch/utils/data/_utils/fetch.py\", line 52, in fetch\n    return self.collate_fn(data)\n  File \"/home/ken/anaconda3/lib/python3.8/site-packages/torch/utils/data/_utils/collate.py\", line 84, in default_collate\n    return [default_collate(samples) for samples in transposed]\n  File \"/home/ken/anaconda3/lib/python3.8/site-packages/torch/utils/data/_utils/collate.py\", line 84, in <listcomp>\n    return [default_collate(samples) for samples in transposed]\n  File \"/home/ken/anaconda3/lib/python3.8/site-packages/torch/utils/data/_utils/collate.py\", line 64, in default_collate\n    return default_collate([torch.as_tensor(b) for b in batch])\n  File \"/home/ken/anaconda3/lib/python3.8/site-packages/torch/utils/data/_utils/collate.py\", line 56, in default_collate\n    return torch.stack(batch, 0, out=out)\nRuntimeError: stack expects each tensor to be equal size, but got [7] at entry 0 and [13] at entry 1\n"
     ]
    }
   ],
   "source": [
    "batch_size = 64\n",
    "n_epochs = 1\n",
    "\n",
    "train_loader, test_loader, valid_loader = create_datasets(batch_size)\n",
    "\n",
    "# early stopping patience;\n",
    "# validation loss가 개선된 마지막 시간 이후로 얼마나 기다릴지 지정\n",
    "patience = 5\n",
    "\n",
    "model, train_loss, valid_loss = train_model(model, batch_size, patience, n_epochs)   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-10-24T21:19:40.213214Z",
     "iopub.status.busy": "2021-10-24T21:19:40.212962Z",
     "iopub.status.idle": "2021-10-24T21:19:40.227601Z",
     "shell.execute_reply": "2021-10-24T21:19:40.22692Z",
     "shell.execute_reply.started": "2021-10-24T21:19:40.213184Z"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "test_df = pd.read_csv('/home/ken/PycharmProjects/goorm_kaist_nlp/Project/Project1_Text Classification/data/test_no_label.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-10-24T21:19:40.479077Z",
     "iopub.status.busy": "2021-10-24T21:19:40.478846Z",
     "iopub.status.idle": "2021-10-24T21:19:40.483838Z",
     "shell.execute_reply": "2021-10-24T21:19:40.482766Z",
     "shell.execute_reply.started": "2021-10-24T21:19:40.479051Z"
    }
   },
   "outputs": [],
   "source": [
    "test_dataset = test_df['Id']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-10-24T21:19:40.983582Z",
     "iopub.status.busy": "2021-10-24T21:19:40.983364Z",
     "iopub.status.idle": "2021-10-24T21:19:40.988374Z",
     "shell.execute_reply": "2021-10-24T21:19:40.987681Z",
     "shell.execute_reply.started": "2021-10-24T21:19:40.983557Z"
    }
   },
   "outputs": [],
   "source": [
    "def make_id_file_test(tokenizer, test_dataset):\n",
    "    data_strings = []\n",
    "    id_file_data = [tokenizer.encode(sent.lower()) for sent in test_dataset]\n",
    "    for item in id_file_data:\n",
    "        data_strings.append(' '.join([str(k) for k in item]))\n",
    "    return data_strings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-10-24T21:19:41.246804Z",
     "iopub.status.busy": "2021-10-24T21:19:41.246527Z",
     "iopub.status.idle": "2021-10-24T21:19:41.624055Z",
     "shell.execute_reply": "2021-10-24T21:19:41.62344Z",
     "shell.execute_reply.started": "2021-10-24T21:19:41.246776Z"
    }
   },
   "outputs": [],
   "source": [
    "test = make_id_file_test(tokenizer, test_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-10-24T21:12:14.60112Z",
     "iopub.status.busy": "2021-10-24T21:12:14.600749Z",
     "iopub.status.idle": "2021-10-24T21:12:14.607618Z",
     "shell.execute_reply": "2021-10-24T21:12:14.606871Z",
     "shell.execute_reply.started": "2021-10-24T21:12:14.601084Z"
    }
   },
   "outputs": [],
   "source": [
    "test[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-10-24T21:14:39.166445Z",
     "iopub.status.busy": "2021-10-24T21:14:39.166192Z",
     "iopub.status.idle": "2021-10-24T21:14:39.174714Z",
     "shell.execute_reply": "2021-10-24T21:14:39.173561Z",
     "shell.execute_reply.started": "2021-10-24T21:14:39.166415Z"
    }
   },
   "outputs": [],
   "source": [
    "class SentimentTestDataset(object):\n",
    "    def __init__(self, tokenizer, test):\n",
    "        self.tokenizer = tokenizer\n",
    "        self.data = []\n",
    "\n",
    "        for sent in test:\n",
    "            self.data += [self._cast_to_int(sent.strip().split())]\n",
    "\n",
    "    def _cast_to_int(self, sample):\n",
    "        return [int(word_id) for word_id in sample]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        sample = self.data[index]\n",
    "        return np.array(sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-10-24T21:14:39.652006Z",
     "iopub.status.busy": "2021-10-24T21:14:39.651535Z",
     "iopub.status.idle": "2021-10-24T21:14:39.659923Z",
     "shell.execute_reply": "2021-10-24T21:14:39.658938Z",
     "shell.execute_reply.started": "2021-10-24T21:14:39.651965Z"
    }
   },
   "outputs": [],
   "source": [
    "test_dataset = SentimentTestDataset(tokenizer, test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-10-24T21:15:26.018936Z",
     "iopub.status.busy": "2021-10-24T21:15:26.018653Z",
     "iopub.status.idle": "2021-10-24T21:15:26.026633Z",
     "shell.execute_reply": "2021-10-24T21:15:26.02577Z",
     "shell.execute_reply.started": "2021-10-24T21:15:26.018904Z"
    }
   },
   "outputs": [],
   "source": [
    "def collate_fn_style_test(samples):\n",
    "    input_ids = samples\n",
    "    max_len = max(len(input_id) for input_id in input_ids)\n",
    "    sorted_indices = np.argsort([len(input_id) for input_id in input_ids])[::-1]\n",
    "\n",
    "    input_ids = pad_sequence([torch.tensor(input_ids[index]) for index in sorted_indices],\n",
    "                             batch_first=True)\n",
    "    attention_mask = torch.tensor(\n",
    "        [[1] * len(input_ids[index]) + [0] * (max_len - len(input_ids[index])) for index in\n",
    "         sorted_indices])\n",
    "    token_type_ids = torch.tensor([[0] * len(input_ids[index]) for index in sorted_indices])\n",
    "    position_ids = torch.tensor([list(range(len(input_ids[index]))) for index in sorted_indices])\n",
    "\n",
    "    return input_ids, attention_mask, token_type_ids, position_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-10-24T21:18:09.655625Z",
     "iopub.status.busy": "2021-10-24T21:18:09.654771Z",
     "iopub.status.idle": "2021-10-24T21:18:09.661488Z",
     "shell.execute_reply": "2021-10-24T21:18:09.660777Z",
     "shell.execute_reply.started": "2021-10-24T21:18:09.655571Z"
    }
   },
   "outputs": [],
   "source": [
    "test_batch_size = 40\n",
    "test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=test_batch_size,\n",
    "                                          shuffle=False, collate_fn=collate_fn_style_test,\n",
    "                                          num_workers=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-10-24T21:18:09.963863Z",
     "iopub.status.busy": "2021-10-24T21:18:09.963666Z",
     "iopub.status.idle": "2021-10-24T21:18:10.941541Z",
     "shell.execute_reply": "2021-10-24T21:18:10.940662Z",
     "shell.execute_reply.started": "2021-10-24T21:18:09.963839Z"
    }
   },
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    model.eval()\n",
    "    predictions = []\n",
    "    for input_ids, attention_mask, token_type_ids, position_ids in tqdm(test_loader,\n",
    "                                                                        desc='Test',\n",
    "                                                                        position=1,\n",
    "                                                                        leave=None):\n",
    "        input_ids = input_ids.to(device)\n",
    "        attention_mask = attention_mask.to(device)\n",
    "        token_type_ids = token_type_ids.to(device)\n",
    "        position_ids = position_ids.to(device)\n",
    "\n",
    "        output = model(input_ids=input_ids,\n",
    "                       attention_mask=attention_mask,\n",
    "                       token_type_ids=token_type_ids,\n",
    "                       position_ids=position_ids,\n",
    "                       labels=labels)\n",
    "\n",
    "        logits = output.logits\n",
    "        batch_predictions = [0 if example[0] > example[1] else 1 for example in logits]\n",
    "        predictions += batch_predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-10-24T21:19:53.123809Z",
     "iopub.status.busy": "2021-10-24T21:19:53.123553Z",
     "iopub.status.idle": "2021-10-24T21:19:53.130545Z",
     "shell.execute_reply": "2021-10-24T21:19:53.129473Z",
     "shell.execute_reply.started": "2021-10-24T21:19:53.12378Z"
    }
   },
   "outputs": [],
   "source": [
    "test_df['Category'] = predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-10-24T21:22:51.35152Z",
     "iopub.status.busy": "2021-10-24T21:22:51.351227Z",
     "iopub.status.idle": "2021-10-24T21:22:51.429885Z",
     "shell.execute_reply": "2021-10-24T21:22:51.428776Z",
     "shell.execute_reply.started": "2021-10-24T21:22:51.351442Z"
    }
   },
   "outputs": [],
   "source": [
    "test_df.to_csv('submission.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 훈련이 진행되는 과정에 따라 loss를 시각화\n",
    "fig = plt.figure(figsize=(10,8))\n",
    "plt.plot(range(1,len(train_loss)+1),train_loss, label='Training Loss')\n",
    "plt.plot(range(1,len(valid_loss)+1),valid_loss,label='Validation Loss')\n",
    "\n",
    "# validation loss의 최저값 지점을 찾기\n",
    "minposs = valid_loss.index(min(valid_loss))+1\n",
    "plt.axvline(minposs, linestyle='--', color='r',label='Early Stopping Checkpoint')\n",
    "\n",
    "plt.xlabel('epochs')\n",
    "plt.ylabel('loss')\n",
    "plt.ylim(0, 0.5) # 일정한 scale\n",
    "plt.xlim(0, len(train_loss)+1) # 일정한 scale\n",
    "plt.grid(True)\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "fig.savefig('loss_plot.png', bbox_inches = 'tight')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
