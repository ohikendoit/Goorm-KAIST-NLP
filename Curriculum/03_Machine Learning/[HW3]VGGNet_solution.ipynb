{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"[HW3]VGGNet_solution.ipynb","provenance":[],"collapsed_sections":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","metadata":{"id":"jR26RFkwXtvi"},"source":["# **[HW3] VGG Network**\n","1. Convolution Operation\n","2. VGG Network\n","3. Train CIFAR-10 with own VGG\n","4. Train CIFAR-10 with pre-trained VGG\n","\n","이번 실습에서는 Image분류에서 종종 baseline model로 사용되는 VGG-Network를 직접 구현해보는 시간을 갖도록 하겠습니다."]},{"cell_type":"markdown","metadata":{"id":"crVJ36mMlaXP"},"source":["\n","\n","## Import packages"]},{"cell_type":"markdown","metadata":{"id":"zpvlE_XOWS33"},"source":["런타임의 유형을 변경해줍니다.\n","\n","상단 메뉴에서 [런타임]->[런타임유형변경]->[하드웨어가속기]->[GPU]\n","\n","변경 이후 아래의 cell을 실행 시켰을 때, torch.cuda.is_avialable()이 True가 나와야 합니다.\n","\n"]},{"cell_type":"code","metadata":{"id":"cqVdEuPQzMAH","executionInfo":{"status":"ok","timestamp":1631158215093,"user_tz":-540,"elapsed":3850,"user":{"displayName":"한종훈","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"08938039053559457272"}},"outputId":"2f9557f8-cc8d-4971-bcb9-22eb179d2e53","colab":{"base_uri":"https://localhost:8080/"}},"source":["import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","import torchvision\n","import torch.optim as optim\n","from torch.utils import data\n","print(torch.__version__)\n","print(torch.cuda.is_available())"],"execution_count":1,"outputs":[{"output_type":"stream","name":"stdout","text":["1.9.0+cu102\n","True\n"]}]},{"cell_type":"code","metadata":{"id":"2o3-HPdHLZma","executionInfo":{"status":"ok","timestamp":1631158218214,"user_tz":-540,"elapsed":1014,"user":{"displayName":"한종훈","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"08938039053559457272"}}},"source":["import matplotlib.pyplot as plt\n","import numpy as np\n","import scipy as sp\n","import tqdm\n","from sklearn.datasets import fetch_openml\n","from sklearn.model_selection import train_test_split\n","\n","np.set_printoptions(precision=3)\n","np.set_printoptions(suppress=True)"],"execution_count":2,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"T1GnKJCB4T_Q"},"source":["# 1. Convolution Operation\n","\n","PyTorch에서는 [nn.sequential](https://pytorch.org/docs/stable/generated/torch.nn.Sequential.html#torch.nn.Sequential) 등의 함수를 이용해서 복잡한 모델 구조를 종종 축약해서 사용하곤 합니다.\n","\n","아래의 예제는 conv-relu-maxpool의 model을 서로 다른 방법으로 표현한 것입니다."]},{"cell_type":"code","metadata":{"id":"EPfV0OTc4Xdr","executionInfo":{"status":"ok","timestamp":1631158219985,"user_tz":-540,"elapsed":2,"user":{"displayName":"한종훈","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"08938039053559457272"}},"outputId":"043f4dca-8ece-41c5-913d-bf0ddc851bb6","colab":{"base_uri":"https://localhost:8080/"}},"source":["input_image = torch.rand(64, 3, 32, 32)\n","input_image.shape"],"execution_count":3,"outputs":[{"output_type":"execute_result","data":{"text/plain":["torch.Size([64, 3, 32, 32])"]},"metadata":{},"execution_count":3}]},{"cell_type":"markdown","metadata":{"id":"l9r2vCuWHX1r"},"source":["### nn.Sequential"]},{"cell_type":"code","metadata":{"id":"FKqcfL4_qK6Q","executionInfo":{"status":"ok","timestamp":1631158221983,"user_tz":-540,"elapsed":1,"user":{"displayName":"한종훈","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"08938039053559457272"}}},"source":["class Conv1(nn.Module):\n","    def __init__(self): # input image = batch_size x 3 x 32 x 32\n","        super(Conv1, self).__init__()\n","        self.conv = nn.Conv2d(3, 64, kernel_size=3, padding=1)\n","        self.relu = nn.ReLU()\n","        self.maxpool = nn.MaxPool2d(2)\n","\n","    def forward(self, x):\n","        x = self.conv(x)\n","        x = self.relu(x)\n","        x = self.maxpool(x)\n","        return x\n","\n","\n","class Conv2(nn.Module):\n","    def __init__(self): # input image = batch_size x 3 x 32 x 32\n","        super(Conv2, self).__init__()\n","        self.layer = nn.Sequential(\n","            nn.Conv2d(3, 64, kernel_size=3, padding=1),\n","            nn.ReLU(),\n","            nn.MaxPool2d(2))\n","\n","    def forward(self, x):\n","        x = self.layer(x)\n","        return x\n","\n","\n","class Conv3(nn.Module):\n","    def __init__(self): # input image = batch_size x 3 x 32 x 32\n","        super(Conv3, self).__init__()\n","        layer = []\n","        layer.append(nn.Conv2d(3, 64, kernel_size=3, padding=1))\n","        layer.append(nn.ReLU())\n","        layer.append(nn.MaxPool2d(2))\n","        self.layer = nn.Sequential(*layer)\n","\n","    def forward(self, x):\n","        x = self.layer(x)\n","        return x\n","\n","model1 = Conv1()\n","model2 = Conv2()\n","model3 = Conv3()"],"execution_count":4,"outputs":[]},{"cell_type":"code","metadata":{"id":"mgD1bTOzqK-n","executionInfo":{"status":"ok","timestamp":1631158222652,"user_tz":-540,"elapsed":2,"user":{"displayName":"한종훈","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"08938039053559457272"}},"outputId":"4dd77b00-b97d-408c-fe65-6a7c606f9896","colab":{"base_uri":"https://localhost:8080/"}},"source":["print(model1)\n","output = model1(input_image)\n","print(output.size())"],"execution_count":5,"outputs":[{"output_type":"stream","name":"stdout","text":["Conv1(\n","  (conv): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","  (relu): ReLU()\n","  (maxpool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",")\n","torch.Size([64, 64, 16, 16])\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:718: UserWarning: Named tensors and all their associated APIs are an experimental feature and subject to change. Please do not use them for anything important until they are released as stable. (Triggered internally at  /pytorch/c10/core/TensorImpl.h:1156.)\n","  return torch.max_pool2d(input, kernel_size, stride, padding, dilation, ceil_mode)\n"]}]},{"cell_type":"code","metadata":{"id":"DVKoXvlYryMK","executionInfo":{"status":"ok","timestamp":1631158224991,"user_tz":-540,"elapsed":228,"user":{"displayName":"한종훈","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"08938039053559457272"}},"outputId":"2d9c794c-b8e1-4a64-f75e-dafd3d3eab96","colab":{"base_uri":"https://localhost:8080/"}},"source":["print(model2)\n","output = model2(input_image)\n","print(output.size())"],"execution_count":8,"outputs":[{"output_type":"stream","name":"stdout","text":["Conv2(\n","  (layer): Sequential(\n","    (0): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","    (1): ReLU()\n","    (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n","  )\n",")\n","torch.Size([64, 64, 16, 16])\n"]}]},{"cell_type":"code","metadata":{"id":"z0wcOPU_ryOg","executionInfo":{"status":"ok","timestamp":1631158226109,"user_tz":-540,"elapsed":238,"user":{"displayName":"한종훈","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"08938039053559457272"}},"outputId":"1721cccc-f289-4906-b279-fba80a336033","colab":{"base_uri":"https://localhost:8080/"}},"source":["print(model3)\n","output = model3(input_image)\n","print(output.size())"],"execution_count":9,"outputs":[{"output_type":"stream","name":"stdout","text":["Conv3(\n","  (layer): Sequential(\n","    (0): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","    (1): ReLU()\n","    (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n","  )\n",")\n","torch.Size([64, 64, 16, 16])\n"]}]},{"cell_type":"markdown","metadata":{"id":"xlkMXZfKxpRg"},"source":["### Let's practice to calculate the shape of the network"]},{"cell_type":"code","metadata":{"id":"fCDbH1Bbxify","executionInfo":{"status":"ok","timestamp":1631158227739,"user_tz":-540,"elapsed":210,"user":{"displayName":"한종훈","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"08938039053559457272"}}},"source":["class Conv(nn.Module):\n","    def __init__(self): # input image = batch_size x 3 x 32 x 32\n","        super(Conv, self).__init__()\n","        self.conv1 = nn.Conv2d(3, 512, 3, 1, 1) \n","        self.conv2 = nn.Conv2d(512, 256, 3, 1, 1) \n","        self.conv3 = nn.Conv2d(256, 256, 3, 2, 1) \n","        ## linear layer의 shape에 맞게 convolution layer의 configuration을 setting하세요. \n","        ## YOUR CODE HERE (~ 1 line)\n","        self.conv4 = nn.Conv2d(256, 256, 3, 4, 1)\n","        ## END OF YOUR CODE\n","        self.linear = nn.Linear(256*4*4, 10)\n","\n","    def forward(self, x):\n","        out=self.conv1(x)\n","        out=self.conv2(out)\n","        out=self.conv3(out)\n","        out=self.conv4(out)\n","        out = out.contiguous().view(-1, 256*4*4)\n","        out = self.linear(out)\n","        return out"],"execution_count":10,"outputs":[]},{"cell_type":"code","metadata":{"id":"Lchz9vtUxkiD","executionInfo":{"status":"ok","timestamp":1631158231313,"user_tz":-540,"elapsed":2949,"user":{"displayName":"한종훈","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"08938039053559457272"}},"outputId":"5892eac3-ffa3-4080-c6bd-946d717ccd7d","colab":{"base_uri":"https://localhost:8080/"}},"source":["model = Conv()\n","print(model)\n","output = model(input_image)\n","print(output.size())"],"execution_count":11,"outputs":[{"output_type":"stream","name":"stdout","text":["Conv(\n","  (conv1): Conv2d(3, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","  (conv2): Conv2d(512, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","  (conv3): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n","  (conv4): Conv2d(256, 256, kernel_size=(3, 3), stride=(4, 4), padding=(1, 1))\n","  (linear): Linear(in_features=4096, out_features=10, bias=True)\n",")\n","torch.Size([64, 10])\n"]}]},{"cell_type":"markdown","metadata":{"id":"b1nhBnqWxw4a"},"source":["# 2. VGG Network\n","\n","이번 section에서는 3*3의 filter로 이루어진 19-layer VGG-network를 직접 구현해보도록 하겠습니다.\n","\n","자세한 모델의 configuration은 아래 그림의 option E와 같습니다.\n","\n","![](https://drive.google.com/uc?export=view&id=1mFwqxP5rB4lhEfRk7OJla1wffKqEibQy)\n","\n","\n","VGG-19는 2layer로 구성된 2개의 convolution block과 4 layer로 구성된 3개의 convolution block으로 나누어져있습니다.\n","\n","또한, 학습 안정성을 위하여 각 layer는 convolution-batchnorm-relu로 이루어지게 되며 매 block에 끝마다 2*2 max pooling을 진행해주게 됩니다. \n","\n"]},{"cell_type":"code","metadata":{"id":"ssBO9DfqagW8"},"source":["class TwoLayerBlock(nn.Module):\n","    def __init__(self, input_dim, output_dim):\n","        super(TwoLayerBlock, self).__init__()\n","        self.in_dim = input_dim\n","        self.out_dim = output_dim\n","        self.layer = nn.Sequential(nn.Conv2d(self.in_dim, self.out_dim, kernel_size=3, padding=1),                    \n","                                  nn.BatchNorm2d(self.out_dim),\n","                                  nn.ReLU(),\n","                                  nn.Conv2d(self.out_dim, self.out_dim, kernel_size=3, padding=1),                    \n","                                  nn.BatchNorm2d(self.out_dim),\n","                                  nn.ReLU(),\n","                                  nn.MaxPool2d(2)\n","                                 )\n","        \n","    def forward(self, x):\n","        x = self.layer(x)\n","        return x"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"YDNAysVqxxOk"},"source":["class FourLayerBlock(nn.Module):\n","    def __init__(self, input_dim, output_dim):\n","        super(FourLayerBlock, self).__init__()\n","        self.in_dim = input_dim\n","        self.out_dim = output_dim\n","        self.layer = nn.Sequential(nn.Conv2d(self.in_dim, self.out_dim, kernel_size=3, padding=1),                    \n","                                  nn.BatchNorm2d(self.out_dim),\n","                                  nn.ReLU(),\n","                                  nn.Conv2d(self.out_dim, self.out_dim, kernel_size=3, padding=1),                    \n","                                  nn.BatchNorm2d(self.out_dim),\n","                                  nn.ReLU(),\n","                                  nn.Conv2d(self.out_dim, self.out_dim, kernel_size=3, padding=1),                    \n","                                  nn.BatchNorm2d(self.out_dim),\n","                                  nn.ReLU(),\n","                                  nn.Conv2d(self.out_dim, self.out_dim, kernel_size=3, padding=1),                    \n","                                  nn.BatchNorm2d(self.out_dim),\n","                                  nn.ReLU(),\n","                                  nn.MaxPool2d(2)\n","                                 )\n","        \n","    def forward(self, x):\n","        x = self.layer(x)\n","        return x"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"l0U2s0hux_n6"},"source":["class VGG19(nn.Module):\n","    def __init__(self):\n","        super(VGG19, self).__init__()\n","        \n","        self.block1 = TwoLayerBlock(3, 64)\n","        self.block2 = TwoLayerBlock(64, 128)\n","        self.block3 = FourLayerBlock(128, 256)\n","        self.block4 = FourLayerBlock(256, 512)\n","        self.block5 = FourLayerBlock(512, 512)\n","        self.linear = nn.Sequential(\n","            nn.Linear(512, 512),\n","            nn.ReLU(),\n","            nn.Linear(512, 128),\n","            nn.ReLU(),\n","            nn.Linear(128, 10),\n","        )\n","    def forward(self, x):\n","        x = self.block1(x)\n","        x = self.block2(x)\n","        x = self.block3(x)\n","        x = self.block4(x)\n","        x = self.block5(x)\n","        x = x.squeeze() \n","        x = self.linear(x)\n","        return x"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"9VSAzG4uz_4i"},"source":["model = VGG19()\n","print(model)\n","\n","output = model(input_image)\n","print(output.size())"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"YOkeC32PaASK"},"source":["### Check Implementations"]},{"cell_type":"code","metadata":{"id":"2B-6gww_Af0E"},"source":["def count_parameters(model):\n","    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n","if count_parameters(model) == 20365002:\n","    print('success!')\n","else:\n","  raise AssertionError"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"36LwdYMDatum"},"source":["# 3. Train CIFAR-10 with own VGG\n","\n","train-image: 50,000\n","test-image: 10,000\n","\n","class: [비행기, 자동차, 트럭, 개구리, ...] 등 10개의 class\n","\n","for more info, https://www.cs.toronto.edu/~kriz/cifar.html"]},{"cell_type":"markdown","metadata":{"id":"g8SWwxvFbK4Q"},"source":["### Use pre-defined dataset\n","\n","PyTorch는 custom dataset과 dataloader를 사용해도 되지만 cifar-10과 같은 유명 데이터셋에 대해서는 pre-defined된 dataset이 존재합니다.\n","\n","이번 실습에서는 custom dataset을 직접 만드는 대신 pre-trained dataset을 불러와서 실습을 진행해보도록 하겠습니다."]},{"cell_type":"code","metadata":{"id":"wrjCzsJ6a0JF"},"source":["import torchvision.datasets as datasets\n","import torchvision.transforms as transforms\n","\n","train_dataset = datasets.CIFAR10(root='./data/',      \n","                                 train=True, \n","                                 transform=transforms.ToTensor(),\n","                                 download=True)\n","\n","test_dataset = datasets.CIFAR10(root='./data/',\n","                                train=False, \n","                                transform=transforms.ToTensor())\n","\n","batch_size = 64\n","train_loader = torch.utils.data.DataLoader(dataset=train_dataset,\n","                                           batch_size=batch_size, \n","                                           shuffle=True)\n","test_loader = torch.utils.data.DataLoader(dataset=test_dataset,\n","                                          batch_size=batch_size, \n","                                          shuffle=False)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"v2l_mwzEa0LK"},"source":["class Trainer():\n","    def __init__(self, trainloader, testloader, model, optimizer, criterion, device):\n","        \"\"\"\n","        trainloader: train data's loader\n","        testloader: test data's loader\n","        model: model to train\n","        optimizer: optimizer to update your model\n","        criterion: loss function\n","        \"\"\"\n","        self.trainloader = trainloader\n","        self.testloader = testloader\n","        self.model = model\n","        self.optimizer = optimizer\n","        self.criterion = criterion\n","        self.device = device\n","        \n","    def train(self, epoch = 1):\n","        self.model.train()\n","        loss_list = []\n","        acc_list = []\n","        for e in range(epoch):\n","            running_loss, running_acc = 0.0, 0.0\n","            for i, data in tqdm.tqdm(enumerate(self.trainloader, 0)): \n","                inputs, labels = data \n","                # model에 input으로 tensor를 gpu-device로 보낸다\n","                inputs = inputs.to(self.device)  \n","                labels = labels.to(self.device)\n","                # zero the parameter gradients\n","                self.optimizer.zero_grad()    \n","                # forward + backward + optimize\n","                outputs = self.model(inputs) \n","                loss = self.criterion(outputs, labels)  \n","                loss.backward() \n","                self.optimizer.step() \n","                running_loss += loss.item()\n","                pred = outputs.max(1, keepdim=True)[1]\n","                running_acc += pred.eq(labels.view_as(pred)).sum().item()\n","            \n","            running_loss = running_loss / len(self.trainloader)\n","            running_acc = running_acc / len(self.trainloader.dataset)\n","            loss_list.append(running_loss)\n","            acc_list.append(running_acc)\n","            print('epoch: %d  loss: %.3f  acc:%.3f' % (e + 1, running_loss, running_acc))\n","            \n","        return loss_list, acc_list\n","\n","    def test(self):\n","        self.model.eval() \n","        correct = 0\n","        for inputs, labels in self.testloader:\n","            inputs = inputs.to(self.device)\n","            labels = labels.to(self.device)\n","            output = self.model(inputs) \n","            pred = output.max(1, keepdim=True)[1] # get the index of the max \n","            correct += pred.eq(labels.view_as(pred)).sum().item()\n","        test_acc = correct / len(self.testloader.dataset)\n","        print('test_acc: %.3f' %(test_acc))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"SAsnxxKka0Nh"},"source":["model = VGG19()\n","criterion = nn.CrossEntropyLoss()\n","optimizer = optim.Adam(model.parameters(), lr=1e-4)\n","device = torch.device('cuda')\n","model.to(device)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"_mX9cm2ca0QL"},"source":["trainer = Trainer(trainloader = train_loader,\n","                  testloader = test_loader,\n","                  model = model,\n","                  criterion = criterion,\n","                  optimizer = optimizer,\n","                  device = device)\n","\n","loss_list, acc_list = trainer.train(epoch = 10)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"X0EAM3MthbCC"},"source":["x = np.arange(10)\n","\n","fig, axs = plt.subplots(1, 2)\n","axs[0].plot(x, loss_list)\n","axs[0].set_xlabel('epoch')\n","axs[0].set_title('loss_plot')\n","\n","axs[1].plot(x, acc_list)\n","axs[1].set_xlabel('epoch')\n","axs[1].set_title('acc_plot')\n","\n","plt.tight_layout()\n","plt.show()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"McNqg46MXztq"},"source":["trainer.test()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"DB76hd60gtP8"},"source":["### Question) 왜 test-accuracy가 train-accuracy에 비해 현저히 낮은 성능을 보일까요?\n","\n","ans) over-fitting"]},{"cell_type":"markdown","metadata":{"id":"WdeY322AZHly"},"source":["# 4. Train CIFAR-10 with pre-trained VGG\n","\n","비록 저희가 직접 학습한 VGG-network를 통해 reasonable한 accuracy를 달성할 수 있었지만, 더 큰 모델을 사용할 경우 매번 처음부터 학습을 진행하는 것은 쉬운 일이 아닙니다. \n","\n","따라서, 이번 섹션에서는 pre-trained된 VGG model을 불러와서 linear layer만 추가적으로 학습시켜 적은 시간의 학습 만으로도 높은 성능을 달성해보는 실습을 진행해보겠습니다.\n","\n","PyTorch에서 공식적으로 제공하는 pre-trained model은 https://pytorch.org/vision/stable/models.html 해당 document에서 찾아볼 수 있으며, 종종 google이나 facebook등에서는 자신들이 개발한 모델에 대해서 pre-trained된 weight를 https://github.com/google-research/simclr 이와 같이 배포하고는 합니다."]},{"cell_type":"code","metadata":{"id":"G6BIBwlUahDn"},"source":["import torchvision.models as models\n","pretrained_vgg = models.vgg19_bn(pretrained=True)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Amx7JWUBZu_C"},"source":["class Pretrained_VGG19(nn.Module):\n","    def __init__(self, pretrained_model):\n","        super(Pretrained_VGG19, self).__init__()\n","        # inherit the weights from the pre-trained model\n","        self.features = nn.Sequential(\n","            *list(pretrained_model.features.children())\n","        )\n","        self.linear = nn.Sequential(\n","            nn.Linear(512, 512),\n","            nn.ReLU(),\n","            nn.Linear(512, 128),\n","            nn.ReLU(),\n","            nn.Linear(128, 10),\n","        )\n","    def forward(self, x):\n","        x = self.features(x)\n","        x = x.squeeze() \n","        x = self.linear(x)\n","        return x"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"frU9_CoBakwL"},"source":["model = Pretrained_VGG19(pretrained_vgg)\n","criterion = nn.CrossEntropyLoss()\n","optimizer = optim.Adam(model.parameters(), lr=1e-4)\n","device = torch.device('cuda')\n","model.to(device)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"bFP-6db_a1ph"},"source":["trainer = Trainer(trainloader = train_loader,\n","                  testloader = test_loader,\n","                  model = model,\n","                  criterion = criterion,\n","                  optimizer = optimizer,\n","                  device = device)\n","\n","loss_list, acc_list = trainer.train(epoch = 1)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"1MlyQXpWa2oU"},"source":["trainer.test()"],"execution_count":null,"outputs":[]}]}