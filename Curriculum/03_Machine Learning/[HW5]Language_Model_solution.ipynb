{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"[HW5]Language_Model_solution.ipynb","provenance":[],"collapsed_sections":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","metadata":{"id":"jR26RFkwXtvi"},"source":["# **[HW5] Language Model**\n","1. DataLoader\n","2. Model\n","3. Trainer\n","4. Generation\n","\n","이번 실습에서는 RNN기반의 Language Model를 구현해서 텍스트를 직접 생성해보는 실습을 진행해보겠습니다.\n","\n","- dataset: WikiText2 (https://github.com/pytorch/examples/tree/master/word_language_model/data/wikitext-2)\n","- model: LSTM\n"]},{"cell_type":"markdown","metadata":{"id":"crVJ36mMlaXP"},"source":["\n","\n","## Import packages"]},{"cell_type":"markdown","metadata":{"id":"zpvlE_XOWS33"},"source":["런타임의 유형을 변경해줍니다.\n","\n","상단 메뉴에서 [런타임]->[런타임유형변경]->[하드웨어가속기]->[GPU]\n","\n","변경 이후 아래의 cell을 실행 시켰을 때, torch.cuda.is_avialable()이 True가 나와야 합니다.\n","\n"]},{"cell_type":"code","metadata":{"id":"cqVdEuPQzMAH","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1631088075388,"user_tz":-540,"elapsed":4431,"user":{"displayName":"hi choi","photoUrl":"","userId":"09784555200355646657"}},"outputId":"7d688fe3-dbde-44dd-8704-981f5bec9bfd"},"source":["import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","import torchvision\n","import torch.optim as optim\n","print(torch.__version__)\n","print(torch.cuda.is_available())"],"execution_count":1,"outputs":[{"output_type":"stream","name":"stdout","text":["1.9.0+cu102\n","True\n"]}]},{"cell_type":"code","metadata":{"id":"2o3-HPdHLZma","executionInfo":{"status":"ok","timestamp":1631088075390,"user_tz":-540,"elapsed":17,"user":{"displayName":"hi choi","photoUrl":"","userId":"09784555200355646657"}}},"source":["import matplotlib.pyplot as plt\n","import numpy as np\n","import scipy as sp\n","import tqdm\n","import os\n","import random\n","import time\n","import datetime\n","\n","# for reproducibility\n","random.seed(1234)\n","np.random.seed(1234)\n","torch.backends.cudnn.deterministic = True\n","torch.backends.cudnn.benchmark = False"],"execution_count":2,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"T1GnKJCB4T_Q"},"source":["# 1. DataLoader\n","\n","이전의 실습들에서 사용한것과 마찬가지로, PyTorch style의 dataloader를 먼저 만들어 두겠습니다."]},{"cell_type":"markdown","metadata":{"id":"wcNl0aWbS0OA"},"source":["### Dataset\n","\n","저희가 이번 실습에서 사용할 데이터셋은 Wikipedia에 있는 영문 글들을 가져온 WikiTree dataset입니다.\n","저희가 불러올 데이터는 가장 작은 WikiTree dataset에서 자주 사용되지 않는 단어나 영어가 아닌 단어들은 <unk>으로 이미 전처리가 되어있습니다."]},{"cell_type":"code","metadata":{"id":"CKf8zNuISiC2","executionInfo":{"status":"ok","timestamp":1631088076155,"user_tz":-540,"elapsed":779,"user":{"displayName":"hi choi","photoUrl":"","userId":"09784555200355646657"}}},"source":["import urllib\n","with urllib.request.urlopen('https://raw.githubusercontent.com/yunjey/pytorch-tutorial/master/tutorials/02-intermediate/language_model/data/train.txt') as f:\n","    data = f.readlines()"],"execution_count":3,"outputs":[]},{"cell_type":"code","metadata":{"id":"jBLNOlRKSpOI","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1631088076157,"user_tz":-540,"elapsed":63,"user":{"displayName":"hi choi","photoUrl":"","userId":"09784555200355646657"}},"outputId":"186892e6-e2bd-4102-caf8-3f49f7cdefab"},"source":["print('num_sentence:',len(data))\n","data[100]"],"execution_count":4,"outputs":[{"output_type":"stream","name":"stdout","text":["num_sentence: 42068\n"]},{"output_type":"execute_result","data":{"text/plain":["b\" plans that give advertisers discounts for maintaining or increasing ad spending have become permanent <unk> at the news <unk> and underscore the fierce competition between newsweek time warner inc. 's time magazine and <unk> b. <unk> 's u.s. news & world report \\n\""]},"metadata":{},"execution_count":4}]},{"cell_type":"code","metadata":{"id":"OfLTv1EPbSwj","colab":{"base_uri":"https://localhost:8080/","height":265},"executionInfo":{"status":"ok","timestamp":1631088076162,"user_tz":-540,"elapsed":40,"user":{"displayName":"hi choi","photoUrl":"","userId":"09784555200355646657"}},"outputId":"d417635e-c1e5-4073-9321-c629c4338abc"},"source":["seq_length_list = []\n","for line in data:\n","    seq_length_list.append(len(line.split()))\n","\n","counts, bins = np.histogram(seq_length_list, bins=20)\n","plt.hist(bins[:-1], bins, weights=counts)\n","plt.show()"],"execution_count":5,"outputs":[{"output_type":"display_data","data":{"image/png":"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\n","text/plain":["<Figure size 432x288 with 1 Axes>"]},"metadata":{"needs_background":"light"}}]},{"cell_type":"markdown","metadata":{"id":"4SdattmOcRwC"},"source":["데이터에 있는 문장 길이들의 histogram을 볼 때 대부분의 data의 문장 길이가 50에 미치지 못하기 때문에 \\\\\n","model에 집어넣을 최대 문장 길이를 50으로 세팅해두도록 하겠습니다."]},{"cell_type":"code","metadata":{"id":"g7MuFqsKcd4U","executionInfo":{"status":"ok","timestamp":1631088076165,"user_tz":-540,"elapsed":35,"user":{"displayName":"hi choi","photoUrl":"","userId":"09784555200355646657"}}},"source":["max_seq_len = 50"],"execution_count":6,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"IyMpsyX8TwYy"},"source":["### Build Dictionary\n","\n","먼저 text 데이터를 모델에 넣어주기 위해서는 text에 존재하는 단어들을 index로 변환해주어야 합니다.\n","\n","이를 위해서는 단어를 index로 변환해주는 word2idx dictionary와 다시 index를 단어로 변환해주는 idx2word dictionary를 만들어야 합니다.\n"]},{"cell_type":"code","metadata":{"id":"cZmyZhcpTvZz","executionInfo":{"status":"ok","timestamp":1631088076481,"user_tz":-540,"elapsed":348,"user":{"displayName":"hi choi","photoUrl":"","userId":"09784555200355646657"}}},"source":["def build_dictionary(data, max_seq_len):\n","    word2idx = {}\n","    idx2word = {}\n","    ## Build Dictionary\n","    word2idx['<pad>'] = 0\n","    word2idx['<unk>'] = 1\n","    idx2word[0] = '<pad>'\n","    idx2word[1] = '<unk>'\n","    idx = 2\n","    for line in data:\n","        words = line.decode('utf-8').split()\n","        words = words[:max_seq_len]        \n","        ### Build Dictionary to convert word to index and index to word\n","        ### YOUR CODE HERE (~ 5 lines)\n","        for word in words:\n","            if word not in word2idx:\n","                word2idx[word] = idx\n","                idx2word[idx] = word\n","                idx += 1\n","\n","    return word2idx, idx2word\n","\n","word2idx, idx2word = build_dictionary(data, max_seq_len)"],"execution_count":7,"outputs":[]},{"cell_type":"code","metadata":{"id":"EPfV0OTc4Xdr","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1631088076483,"user_tz":-540,"elapsed":37,"user":{"displayName":"hi choi","photoUrl":"","userId":"09784555200355646657"}},"outputId":"60f2dd2f-8048-483e-9ff8-891b22ddb3cb"},"source":["if len(word2idx) == len(idx2word) == 10000:\n","    print(\"Test Passed!\")\n","else:\n","    raise AssertionError"],"execution_count":8,"outputs":[{"output_type":"stream","name":"stdout","text":["Test Passed!\n"]}]},{"cell_type":"markdown","metadata":{"id":"me_m8njoXHrv"},"source":["### Preprocessing\n","\n","이제 앞서 만든 dictionary를 이용해서 text로된 데이터셋을 index들로 변환시키겠습니다."]},{"cell_type":"code","metadata":{"id":"I6fuARgzXEDU","executionInfo":{"status":"ok","timestamp":1631088077213,"user_tz":-540,"elapsed":747,"user":{"displayName":"hi choi","photoUrl":"","userId":"09784555200355646657"}}},"source":["def preprocess(data, word2idx, idx2word, max_seq_len):\n","    tokens = []\n","    for line in data:\n","        words = line.decode('utf-8').split()\n","        words = words[:max_seq_len]\n","        ### Convert dataset with tokens\n","        ### For each line, append <pad> token to match the number of max_seq_len\n","        ### YOUR CODE HERE (~ 4 lines)\n","        words += ['<pad>']*(max_seq_len - len(words))\n","        for word in words:\n","            token = word2idx[word]\n","            tokens.append(token)\n","\n","    return tokens\n","\n","tokens = preprocess(data, word2idx, idx2word, max_seq_len)"],"execution_count":9,"outputs":[]},{"cell_type":"code","metadata":{"id":"VjyvqMgbZnfP","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1631088077215,"user_tz":-540,"elapsed":43,"user":{"displayName":"hi choi","photoUrl":"","userId":"09784555200355646657"}},"outputId":"e1f845d3-884b-4e7e-825d-756d1bdd7545"},"source":["if len(tokens) == 2103400:\n","    print(\"Test Passed!\")\n","else:\n","    raise AssertionError"],"execution_count":10,"outputs":[{"output_type":"stream","name":"stdout","text":["Test Passed!\n"]}]},{"cell_type":"markdown","metadata":{"id":"jmQxX3BH-SAv"},"source":["이제 전처리된 Token들을 문장 단위의 배열로 변환시켜 두겠습니다."]},{"cell_type":"code","metadata":{"id":"knMvtp23-Jye","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1631088077217,"user_tz":-540,"elapsed":40,"user":{"displayName":"hi choi","photoUrl":"","userId":"09784555200355646657"}},"outputId":"a884c12d-1f24-442a-c4a2-b90d19626193"},"source":["tokens = np.array(tokens).reshape(-1, max_seq_len)\n","print(tokens.shape)\n","tokens[100]"],"execution_count":11,"outputs":[{"output_type":"stream","name":"stdout","text":["(42068, 50)\n"]},{"output_type":"execute_result","data":{"text/plain":["array([745,  93, 746, 739, 747, 181, 748, 467, 749, 740, 750, 154, 751,\n","       752,   1, 160,  32, 753,   1,  48, 754,  32, 755, 756, 757, 728,\n","       555, 758,  99, 119, 555, 733,  48,   1, 759,   1, 119, 237, 753,\n","       230, 760, 347,   0,   0,   0,   0,   0,   0,   0,   0])"]},"metadata":{},"execution_count":11}]},{"cell_type":"markdown","metadata":{"id":"pceBqmtTZ9g9"},"source":["### DataLoader\n","\n","이제 전처리된 dataset을 활용하여 PyTorch style의 dataset과 dataloader를 만들도록 하겠습니다.\n","\n","Token형태의 데이터를 PyTorch 스타일의 dataset으로 만들 때 주의할 점은, 추후 embedding matrix에서 indexing을 해주기 위해서 각 token이 LongTensor 형태로 정의되어야 한다는 점입니다."]},{"cell_type":"code","metadata":{"id":"1hAwhG1K9iBI","executionInfo":{"status":"ok","timestamp":1631088077220,"user_tz":-540,"elapsed":32,"user":{"displayName":"hi choi","photoUrl":"","userId":"09784555200355646657"}}},"source":["class LMDataset(torch.utils.data.Dataset):\n","    def __init__(self, tokens):\n","        super(LMDataset, self).__init__()\n","        self.PAD = 0\n","        self.UNK = 1\n","        self.tokens = tokens\n","        self._getitem(2)\n","\n","    def _getitem(self, index):\n","        X = self.tokens[index]\n","        y = np.concatenate((X[1:], [self.PAD]))\n","\n","        X = torch.from_numpy(X).unsqueeze(0).long()\n","        y = torch.from_numpy(y).unsqueeze(0).long()\n","\n","        return X, y\n","\n","    def __getitem__(self, index):\n","        X = self.tokens[index]\n","        y = np.concatenate((X[1:], [self.PAD]))\n","\n","        X = torch.from_numpy(X).long()\n","        y = torch.from_numpy(y).long()\n","\n","        return X, y\n","\n","    def __len__(self):\n","        return len(self.tokens)"],"execution_count":12,"outputs":[]},{"cell_type":"code","metadata":{"id":"BiLNqM6kAda1","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1631088077221,"user_tz":-540,"elapsed":31,"user":{"displayName":"hi choi","photoUrl":"","userId":"09784555200355646657"}},"outputId":"2403bdbc-2ed3-4f76-9ecc-91b0bc32bbc1"},"source":["batch_size = 64\n","dataset = LMDataset(tokens)\n","dataloader = torch.utils.data.DataLoader(dataset, batch_size=batch_size, shuffle=True)\n","\n","print(len(dataset))\n","print(len(dataloader))"],"execution_count":13,"outputs":[{"output_type":"stream","name":"stdout","text":["42068\n","658\n"]}]},{"cell_type":"markdown","metadata":{"id":"b1nhBnqWxw4a"},"source":["# 2. Model\n","\n","이번 section에서는 Language Modeling을 위한 Recurrent Model을 직접 만들어보도록 하겠습니다.\n","\n","Standard한 Recurrent Neural Network (RNN) model은 vanishing gradient 문제에 취약하기 때문에, 이번 실습에서는 변형된 RNN구조인 LSTM model을 활용하도록 하겠습니다.\n"]},{"cell_type":"markdown","metadata":{"id":"aOoNVt3MDOjl"},"source":["### LSTM"]},{"cell_type":"markdown","metadata":{"id":"9lycT_9vwaJN"},"source":["LSTM model의 전체적인 구조와 각 gate의 수식은 아래와 같습니다.\n","\n","![](https://drive.google.com/uc?export=view&id=1n93tpNW55Xl4GxZNcJcbUVRhuNCGH38h)"]},{"cell_type":"markdown","metadata":{"id":"S1h6nfvYwN8n"},"source":["![](https://drive.google.com/uc?export=view&id=1nH9U5iD9cO6OVVTbrx-LjypRvcWzbOCU)\n","\n","LSTM의 자세한 동작방식이 궁금하신 분은 아래의 블로그를 참조해주세요.\n","\n","https://colah.github.io/posts/2015-08-Understanding-LSTMs/"]},{"cell_type":"code","metadata":{"id":"YDNAysVqxxOk","executionInfo":{"status":"ok","timestamp":1631088077722,"user_tz":-540,"elapsed":522,"user":{"displayName":"hi choi","photoUrl":"","userId":"09784555200355646657"}}},"source":["class LSTMCell(nn.Module):\n","    def __init__(self, input_size, hidden_size):\n","        super(LSTMCell, self).__init__()\n","        # input-gate\n","        self.Wi = nn.Linear(input_size + hidden_size, hidden_size)\n","        # forget-gate\n","        self.Wf = nn.Linear(input_size + hidden_size, hidden_size)\n","        # gate-gate\n","        self.Wg = nn.Linear(input_size + hidden_size, hidden_size)\n","        # output-gate\n","        self.Wo = nn.Linear(input_size + hidden_size, hidden_size)\n","\n","        # non-linearity\n","        self.sigmoid = nn.Sigmoid()\n","        self.tanh = nn.Tanh()\n","\n","    def forward(self, x, h_0, c_0):\n","        \"\"\"\n","        Inputs\n","            input (x): [batch_size, input_size]\n","            hidden_state (h_0): [batch_size, hidden_size]\n","            cell_state (c_0): [batch_size, hidden_size]\n","        Outputs\n","            next_hidden_state (h_1): [batch_size, hidden_size]\n","            next_cell_state (c_1): [batch_size, hidden_size]    \n","        \"\"\"\n","        h_1, c_1 = None, None\n","        input = torch.cat((x, h_0), 1)\n","        # Implement LSTM cell as noted above\n","        ### YOUR CODE HERE (~ 6 lines)\n","        i = self.sigmoid(self.Wi(input))\n","        f = self.sigmoid(self.Wf(input))\n","        g = self.tanh(self.Wg(input))\n","        o = self.sigmoid(self.Wo(input))\n","        c_1 = f * c_0 + i * g\n","        h_1 = o * self.tanh(c_1)\n","\n","        return h_1, c_1"],"execution_count":14,"outputs":[]},{"cell_type":"code","metadata":{"id":"N0Tff2VCJ56D","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1631088077725,"user_tz":-540,"elapsed":35,"user":{"displayName":"hi choi","photoUrl":"","userId":"09784555200355646657"}},"outputId":"676217d6-93e2-4861-c79d-ab427ca693ea"},"source":["def test_lstm():\n","    batch_size = 2\n","    input_size = 5\n","    hidden_size = 3\n","\n","    #torch.manual_seed(1234)\n","    lstm = LSTMCell(input_size ,hidden_size)\n","    def init_weights(m):\n","        if isinstance(m, nn.Linear):\n","            torch.nn.init.constant_(m.weight, 0.1)\n","            m.bias.data.fill_(0.01)\n","    lstm.apply(init_weights)\n","\n","    x = torch.ones(batch_size, input_size)\n","    hx = torch.zeros(batch_size, hidden_size)\n","    cx = torch.zeros(batch_size, hidden_size)\n","\n","    hx, cx = lstm(x, hx, cx)\n","    assert hx.detach().allclose(torch.tensor([[0.1784, 0.1784, 0.1784], \n","                                              [0.1784, 0.1784, 0.1784]]), atol=2e-1), \\\n","            f\"Output of the hidden state does not match.\"\n","    assert cx.detach().allclose(torch.tensor([[0.2936, 0.2936, 0.2936], \n","                                              [0.2936, 0.2936, 0.2936]]), atol=2e-1), \\\n","            f\"Output of the cell state does not match.\"\n","\n","    print(\"==LSTM cell test passed!==\")\n","\n","test_lstm()"],"execution_count":15,"outputs":[{"output_type":"stream","name":"stdout","text":["==LSTM cell test passed!==\n"]}]},{"cell_type":"markdown","metadata":{"id":"0DxU-78B33dG"},"source":["## Language Model\n","\n","이제, 위에서 정의한 LSTM Cell을 활용해서 아래와 같은 Langauge Model을 만들어보도록 하겠습니다.\n","\n","\n","![](https://drive.google.com/uc?export=view&id=1nMAbL-g31nERM44dgohA3k9Vj_92hIh-)"]},{"cell_type":"code","metadata":{"id":"l0U2s0hux_n6","executionInfo":{"status":"ok","timestamp":1631088077728,"user_tz":-540,"elapsed":30,"user":{"displayName":"hi choi","photoUrl":"","userId":"09784555200355646657"}}},"source":["class LanguageModel(nn.Module):\n","    def __init__(self, input_size=64, hidden_size=64, vocab_size=10000):\n","        super(LanguageModel, self).__init__()\n","        \n","        self.input_layer = nn.Embedding(vocab_size, input_size)\n","        self.hidden_layer = LSTMCell(input_size, hidden_size)\n","        self.output_layer = nn.Linear(hidden_size, vocab_size)\n","\n","\n","    def forward(self, x, hx, cx, predict=False):\n","        \"\"\"\n","        Inputs\n","            input (x): [batch_size]\n","            hidden_state (h_0): [batch_size, hidden_size]\n","            cell_state (c_0): [batch_size, hidden_size]\n","            predict: whether to predict and sample the next word\n","        Outputs\n","            output (ox): [batch_size, hidden_size]\n","            next_hidden_state (h_1): [batch_size, hidden_size]\n","            next_cell_state (c_1): [batch_size, hidden_size]    \n","        \"\"\"\n","        x = self.input_layer(x)\n","        hx, cx = self.hidden_layer(x, hx, cx)\n","        ox = self.output_layer(hx)\n","\n","        if predict == True:\n","            probs = F.softmax(ox, dim=1)\n","            # torch distribution allows sampling operation\n","            # see https://pytorch.org/docs/stable/distributions.html\n","            dist = torch.distributions.Categorical(probs)\n","            ox = dist.sample()\n","\n","        return ox, hx, cx  "],"execution_count":16,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"G-ZpuMhsbBS8"},"source":["# 3. Trainer\n","\n","자 이제 위에서 구현한 dataloader와 langauge model을 활용해서 모델의 학습을 진행해보도록 하겠습니다.\n"]},{"cell_type":"code","metadata":{"id":"y7TY7HmvbRlB","executionInfo":{"status":"ok","timestamp":1631088077732,"user_tz":-540,"elapsed":28,"user":{"displayName":"hi choi","photoUrl":"","userId":"09784555200355646657"}}},"source":["class Trainer():\n","    def __init__(self, \n","                 word2idx, \n","                 idx2word,\n","                 dataloader, \n","                 model, \n","                 criterion,\n","                 optimizer, \n","                 device):\n","        \"\"\"\n","        dataloader: dataloader\n","        model: langauge model\n","        criterion: loss function to evaluate the model (e.g., BCE Loss)\n","        optimizer: optimizer for model\n","        \"\"\"\n","        self.word2idx = word2idx\n","        self.idx2word = idx2word\n","        self.dataloader = dataloader\n","        self.model = model\n","        self.criterion = criterion\n","        self.optimizer = optimizer\n","        self.device = device\n","        \n","    def train(self, epochs = 1):\n","        self.model.to(self.device)\n","        start_time = time.time()\n","        for epoch in range(epochs):\n","            losses = []\n","            for iter, (x_batch, y_batch) in tqdm.tqdm(enumerate(self.dataloader)):\n","                self.model.train()\n","                \n","                batch_size, max_seq_len = x_batch.shape\n","                x_batch = x_batch.to(self.device)\n","                y_batch = y_batch.to(self.device)\n","\n","                # initial hidden-states\n","                hx = torch.zeros(batch_size, hidden_size).to(self.device)\n","                cx = torch.zeros(batch_size, hidden_size).to(self.device)\n","\n","                # Implement LSTM operation\n","                ox_batch = []\n","                # Get output logits for each time sequence and append to the list, ox_batch\n","                # YOUR CODE HERE (~ 4 lines)\n","                for s_idx in range(max_seq_len):\n","                    x = x_batch[:, s_idx]\n","                    ox, hx, cx = self.model(x, hx, cx)\n","                    ox_batch.append(ox)\n","                # outputs are ordered by the time sequence\n","                ox_batch = torch.cat(ox_batch).reshape(max_seq_len, batch_size, -1)\n","                ox_batch = ox_batch.permute(1,0,2).reshape(batch_size*max_seq_len, -1)\n","                y_batch = y_batch.reshape(-1)\n","\n","                self.model.zero_grad()\n","                loss = self.criterion(ox_batch, y_batch)\n","                loss.backward()\n","                self.optimizer.step()\n","                losses.append(loss.item())\n","\n","            end_time = time.time() - start_time\n","            end_time = str(datetime.timedelta(seconds=end_time))[:-7]\n","            print('Time [%s], Epoch [%d/%d], loss: %.4f'\n","                  % (end_time, epoch+1, epochs, np.mean(losses)))\n","            if epoch % 5 == 0:\n","                generated_sentences = self.test()\n","                print('[Generated Sentences]')\n","                for sentence in generated_sentences:\n","                    print(sentence)\n","            \n","    def test(self):\n","        # Test model to genereate the sentences\n","        self.model.eval()\n","        num_sentence = 5\n","        max_seq_len = 50\n","\n","        # initial hidden-states\n","        outs = []\n","        x = torch.randint(0, 10000, (num_sentence,)).to(self.device)\n","        hx = torch.zeros(num_sentence, hidden_size).to(self.device)\n","        cx = torch.zeros(num_sentence, hidden_size).to(self.device)\n","\n","        outs.append(x)\n","        with torch.no_grad():\n","            for s_idx in range(max_seq_len-1):\n","                x, hx, cx = self.model(x, hx, cx, predict=True)\n","                outs.append(x)\n","        outs = torch.cat(outs).reshape(max_seq_len, num_sentence)\n","        outs = outs.permute(1, 0)\n","        outs = outs.detach().cpu().numpy()\n","\n","        sentences = []\n","        for out in outs:\n","            sentence = []\n","            for token_idx in out:\n","                word = self.idx2word[token_idx]\n","                sentence.append(word)\n","            sentences.append(sentence)\n","       \n","        return sentences"],"execution_count":17,"outputs":[]},{"cell_type":"code","metadata":{"id":"fgEJv1vWqNkS","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1631089974077,"user_tz":-540,"elapsed":975528,"user":{"displayName":"hi choi","photoUrl":"","userId":"09784555200355646657"}},"outputId":"24d1fffd-49fe-47df-93d2-64d70ccc2864"},"source":["lr = 1e-2\n","input_size = 128\n","hidden_size = 128\n","batch_size = 256\n","\n","dataset = LMDataset(tokens)\n","dataloader = torch.utils.data.DataLoader(dataset, batch_size=batch_size, shuffle=True)\n","model = LanguageModel(input_size=input_size, hidden_size=hidden_size)\n","# NOTE: you should use ignore_index to ignore the loss from predicting the <PAD> token\n","criterion = nn.CrossEntropyLoss(ignore_index=0)\n","optimizer = optim.Adam(model.parameters(), lr=lr)\n","device = torch.device('cuda')\n","\n","trainer = Trainer(word2idx = word2idx,\n","                  idx2word = idx2word,\n","                  dataloader=dataloader, \n","                  model = model,\n","                  criterion=criterion,\n","                  optimizer = optimizer,\n","                  device=device)\n","\n","trainer.train(epochs=50)"],"execution_count":18,"outputs":[{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["165it [00:37,  4.37it/s]\n"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Time [0:00:37], Epoch [1/50], loss: 6.0723\n","[Generated Sentences]\n","['blues', 'firmly', 'irony', 'it', 'had', 'a', 'two-tier', '<unk>', 'year-end', 'so', 'chief', 'executive', 'vice', 'this', 'week', 'said', 'it', 'owns', 'them', 'he', 'really', 'has', 'been', 'in', 'washington', 'did', 'little', 'management', 'know', 'ltd.', 'across', 'the', 'medical', 'markets', 'and', 'us', 'establishing', 'the', 'seven', 'years', 'program', 'themselves', 'to', '<unk>', 'purchase', 'funds', 'into', 'growth', 'and', 'pain']\n","['containers', 'ariz.', 'patents', '<unk>', 'his', 'statement', 'specific', 'caribbean', 'out', 'if', 'fiscal', 'point', 'appears', 'to', 'the', 'buy-out', 'produces', 'and', 'give', 'her', 'nervous', 'including', 'immediate', 'airline', 'for', '<unk>', 'a', '<unk>', 'five', 'years', 'old', 'use', 'and', '<unk>', '<unk>', 'pa.', '<unk>', 'gerald', 'the', 'need', 'a', 'major', 'role', 'will', 'brokerage', 'vice', 'president', 'of', 'the', '<unk>']\n","['attempting', 'engelken', 'appropriate', 'a', 'johns', 'effort', 'a', 'series', 'they', 'have', 'thought', 'just', 'to', 'the', 'bids', 'planner', 'off', 'their', 'bidding', 'bull', 'and', 'tanks', 'by', 'its', '<unk>', 'for', 'west', 'or', '<unk>', 'research', 'inc.', 'i', 'wish', 'japan', 'on', '<unk>', 'the', 'reagan', 'rooms', '<unk>', 'any', 'major', 'issue', '$', 'one', 'gene', 'has', 'in', 'tesoro', 'margins']\n","['types', 'the', '<unk>', 'under', 'a', 'lid', 'for', 'guy', 'have', 'to', 'come', 'at', 'a', 'pattern', 'of', 'san', 'francisco', '<unk>', 'detectors', 'bought', 'foreign', 'cards', 'to', 'ultimately', 'earlier', 'let', 'the', 'u.s.', 'trade', 'aftershocks', 'and', '<unk>', 'colleagues', 'in', 'japan', 'that', 'is', 'available', 'opposed', 'to', 'reach', 'other', 'sciences', 'has', 'become', 'little', 'portfolio', 'to', 'return', 'in']\n","['circulation', '&', 'co', 'presidents', 'are', 'acquiring', '<unk>', 'of', 'webster', 'companies', 'heritage', 'which', 'included', 'out', 'of', 'its', '$', 'N', 'billion', 'yen', 'half', 'program', 'trading', 'at', 'N', 'N', 'to', 'transfer', 'a', 'loss', 'in', 'preparation', 'such', 'or', 'N', 'more', 'expensive', 'or', 'a', 'year', 'in', 'second', 'local', 'network', 'coverage', 'in', 'the', 'chemicals', \"'s\", 'action']\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["165it [00:37,  4.40it/s]\n"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Time [0:01:15], Epoch [2/50], loss: 5.2060\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["165it [00:37,  4.40it/s]\n"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Time [0:01:52], Epoch [3/50], loss: 4.8946\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["165it [00:37,  4.39it/s]\n"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Time [0:02:30], Epoch [4/50], loss: 4.6907\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["165it [00:37,  4.40it/s]\n"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Time [0:03:08], Epoch [5/50], loss: 4.5336\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["165it [00:37,  4.39it/s]\n"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Time [0:03:45], Epoch [6/50], loss: 4.4075\n","[Generated Sentences]\n","['constantly', 'airlines', 'here', 'thinking', 'in', 'a', 'process', '<unk>', 'in', '<unk>', 'n.y', 'free', 'of', 'was', 'nearly', 'N', 'N', 'of', 'the', 'fdic', 'were', 'rising', 'despite', 'down', 'with', 'the', 'name', 'takeover', 'bid', 'should', 'begin', 'by', '<unk>', 'holders', 'because', 'the', 'market', \"'s\", 'plunge', 'between', 'the', 'earnings', 'drop', 'or', '$', 'N', 'a', 'year', 'earlier', 'it']\n","['four-game', 'argue', 'mr.', 'selling', 'pledged', 'to', 'joining', 'them', 'maybe', 'more', 'it', 'or', '<unk>', 'their', 'work', 'it', '<unk>', 'for', 'much', 'of', 'the', 'support', 'task', 'was', '<unk>', 'of', 'stock', 'forces', 'on', 'the', 'matter', 'before', 'his', 'case', 'also', 'was', 'a', 'major', 'present', 'his', '<unk>', 'in', 'science', 'but', 'the', 'request', 'was', 'needed', 'for', 'an']\n","['interviewed', 'canadian', 'banks', 'arise', 'the', 'exchange', 'commission', 'said', 'net', 'income', 'came', 'from', 'a', 'unit', 'of', '$', 'N', 'billion', 'in', 'lilly', \"'s\", 'report', 'in', 'N', 'N', 'from', 'the', 'three', 'outcry', 'of', 'nynex', 'electric', 'co.', 'and', 'sales', 'of', 'the', 'british', 'automotive', 'shares', 'rose', 'slightly', 'from', 'late', 'N', 'million', 'shares', 'according', 'to', 'analysts']\n","['insurance', 'could', 'a', 'general', 'corporate', 'people', 'at', 'basic', 'sort', '<unk>', '<unk>', 'furs', 'and', 'physical', 'policies', '<unk>', 'for', 'congress', 'to', 'bring', 'about', 'other', '<unk>', 'the', 'white', 'house', 'in', 'the', 'safety', 'and', 'mrs.', 'yeargin', 'says', 'he', 'visited', 'the', 'congressman', 'particularly', 'in', 'way', 'for', 'legislation', 'to', 'put', 'fraser', 'with', '<unk>', 'train', 'to', 'planners']\n","['buoyed', 'by', 'at', 'least', 'than', 'N', 'N', 'is', \"n't\", 'expected', 'to', 'allow', 'recession', 'is', 'based', 'on', 'which', 'versions', 'of', 'a', 'slowing', 'economy', 'in', 'japan', 'in', 'order', 'to', 'acquire', 'the', 'u.s.', 'needs', 'the', 'evidence', 'these', 'months', 'were', '<unk>', 'about', 'N', 'in', 'the', 'second', 'half', 'of', 'its', 'big', 'board', 'trading', 'contel', \"'s\"]\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["165it [00:37,  4.40it/s]\n"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Time [0:04:23], Epoch [7/50], loss: 4.2996\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["165it [00:37,  4.40it/s]\n"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Time [0:05:00], Epoch [8/50], loss: 4.2077\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["165it [00:37,  4.38it/s]\n"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Time [0:05:38], Epoch [9/50], loss: 4.1276\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["165it [00:37,  4.39it/s]\n"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Time [0:06:16], Epoch [10/50], loss: 4.0590\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["165it [00:37,  4.40it/s]\n"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Time [0:06:53], Epoch [11/50], loss: 3.9937\n","[Generated Sentences]\n","['catastrophic', 'unilab', 'which', 'has', 'been', 'halted', 'trading', 'at', 'N', 'N', 'to', 'yield', 'N', 'N', 'N', 'N', 'now', 'at', 'light', 'point', 'about', '$', 'N', 'plus', 'accumulated', 'since', 'its', 'latest', 'reports', 'about', 'N', 'employees', 'collapsed', 'ahead', 'in', 'the', 'first', 'few', 'months', 'of', 'sales', 'of', 'shareholders', 'at', 'daf', \"'s\", '190.58-point', 'plunge', 'friday', 'and']\n","['assets', 'trade', 'last', 'year', 'will', 'be', 'used', 'to', 'ballooning', 'because', 'of', 'debt', 'to', 'install', 'power', 'to', 'travel', 'to', 'back', 'shares', 'nearly', 'the', 'smallest', 'bid', 'from', 'industrial', '<unk>', 'group', 'which', 'has', 'launched', 'some', 'N', 'sales', 'of', 'N', 'N', 'N', 'during', 'the', 'option', 'with', 'limited', 'sales', 'for', 's&p', 'N', 'offices', 'and', 'net']\n","['between', 'N', 'mark', 'but', 'an', 'of', 'the', '<unk>', 'drug', 'carries', 'a', 'few', 'people', 'familiar', 'with', 'fake', 'salvador', \"'s\", 'pro', 'late', 'in', 'san', 'francisco', 'was', 'made', 'if', 'that', 'only', 'two-thirds', 'have', 'been', 'more', 'rapidly', 'prone', 'to', 'that', 'over', 'it', '<unk>', 'white', '<unk>', 'up', 'for', 'a', '<unk>', 'from', 'corrupt', 'victims', 'of', 'eggs']\n","['exclusively', 'faced', 'with', 'a', 'group', 'the', 'amount', 'of', 'series', 'mutual', 'markets', 'earlier', 'this', 'year', 'will', 'be', 'used', 'to', 'huge', 'conditions', 'we', 'are', 'talking', 'about', 'politicians', 'and', 'people', 'who', \"'ve\", 'tried', 'altogether', 'to', '<unk>', 'and', 'always', 'go', 'down', 'their', 'way', 'of', 'people', 'but', 'feel', 'sure', 'grace', 'moral', '<unk>', 'questions', 'about', 'this']\n","['single', 'institutions', 'that', 'now', 'like', '<unk>', 'hess', 'had', 'been', 'easing', 'in', 'his', 'concept', 'front', 'said', 'columbia', '<unk>', 'a', '40-year-old', 'publication', '<unk>', 'to', 'her', 'public', 'because', 'they', 'are', 'equal', 'for', 'a', 'bit', 'disappointing', '<unk>', 'representing', 'about', 'N', 'and', 'N', 'N', '<unk>', 'troops', 'said', 'they', '<unk>', 'page', 'what', 'most', 'people', 'ought', 'to']\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["165it [00:37,  4.39it/s]\n"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Time [0:07:31], Epoch [12/50], loss: 3.9382\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["165it [00:37,  4.39it/s]\n"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Time [0:08:08], Epoch [13/50], loss: 3.8866\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["165it [00:37,  4.39it/s]\n"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Time [0:08:46], Epoch [14/50], loss: 3.8402\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["165it [00:37,  4.38it/s]\n"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Time [0:09:24], Epoch [15/50], loss: 3.7990\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["165it [00:37,  4.40it/s]\n"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Time [0:10:01], Epoch [16/50], loss: 3.7603\n","[Generated Sentences]\n","['maintenance', 'addresses', 'enabling', 'kidder', 'peabody', '<unk>', '<unk>', 'inc', 'league', 'last', 'june', 'to', 'about', 'N', 'junk-bond', 'stakes', 'in', 'financial', 'markets', 'inc.', 'has', 'been', 'a', 'long', '<unk>', 'against', 'the', 'dispute', 'that', 'mr.', 'johnson', 'expects', 'the', 'N', '1\\\\/2-year', 'contract', 'was', 'delayed', 'but', 'being', 'acquired', 'workers', '<unk>', 'dropped', 'to', 'mca', 'inc.', \"'s\", '<unk>', 'with']\n","['sink', 'pulling', 'out', 'of', 'one', 'cent', 'by', 'limiting', 'much', 'of', 'the', 'blame', 'on', 'the', 'dominant', 'management', 'team', 'thomas', 'd.', '<unk>', 'of', 'a', 'first-time', '<unk>', 'the', '<unk>', 'went', 'to', 'his', 'village', 'the', 'nations', 'severely', '<unk>', 'lovely', 'in', 'japanese', 'companies', '<unk>', 'produce', 'seed', 'on', 'old', 'vienna', 'but', 'incorrect', 'view', 'course', 'are', 'coming']\n","['small-town', 'subscribers', 'but', 'like', 'leslie', 'i.', '<unk>', 'assistant', 'secretary', 'cheney', 'may', 'seek', 'a', 'bitter', 'vote', 'for', 'both', 'wages', 'and', 'sales', 'of', 'higher', 'industry', 'information', 'to', 'facilitate', 'mr.', '<unk>', 'said', 'but', 'consumer', 'expenditure', 'has', 'been', 'a', 'modest', 'N', 'N', 'interest', 'in', 'which', 'was', '<unk>', 'away', 'from', '<unk>', '<unk>', 'said', 'his', 'board']\n","['demonstrate', 'favor', 'american', 'as', 'so', 'wall', 'is', 'going', 'N', 'N', 'points', 'to', 'be', 'severely', 'damaged', 'and', 'pork', 'words', 'of', 'private', 'investigators', 'from', 'N', 'local', 'magazines', 'on', 'wall', 'street', 'journal', 'group', 'and', 's.', '<unk>', 'of', '<unk>', 'contracts', 'students', 'and', 'confiscated', 'rice', 'highway', 'appliances', 'than', 'they', 'will', 'need', 'to', 'take', 'its', 'guilty']\n","['quickview', 'plc', \"'s\", 'commercial', 'bank', 'as', 'steep', 'have', 'poured', 'it', 'far', 'less', 'than', 'the', 'epa', 'current', 'weight', 'closing', 'advisory', 'unit', '<unk>', 'to', '<unk>', 'by', 'ogden', 'corp.', '<unk>', 'violating', 'securities', 'and', 'exchange', 'commission', 'filings', 'and', 'responsible', 'annual', 'growth', 'including', '<unk>', 'securities', 'including', 'philip', 'morris', 'assets', 'through', 'a', '$', 'N', 'million', 'rise']\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["165it [00:37,  4.38it/s]\n"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Time [0:10:39], Epoch [17/50], loss: 3.7249\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["165it [00:37,  4.38it/s]\n"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Time [0:11:17], Epoch [18/50], loss: 3.6920\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["165it [00:37,  4.39it/s]\n"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Time [0:11:54], Epoch [19/50], loss: 3.6605\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["165it [00:37,  4.38it/s]\n"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Time [0:12:32], Epoch [20/50], loss: 3.6344\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["165it [00:37,  4.40it/s]\n"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Time [0:13:09], Epoch [21/50], loss: 3.6103\n","[Generated Sentences]\n","['palmer', 'haas', 'management', '<unk>', 'the', 'company', \"'s\", 'stock', 'for', 'the', 'third', 'quarter', 'when', 'saatchi', '&', 'saatchi', \"'s\", '$', 'N', 'billion', 'scheduled', 'on', 'by', 'moody', \"'s\", 'and', 's&p', 'rose', 'to', '$', 'N', 'million', 'on', 'more', 'than', 'half', 'of', 'N', 'stores', 'in', 'stock', 'prices', 'and', 'shares', 'will', 'have', 'about', '$', 'N', 'million']\n","['narrower', 'henry', '<unk>', 'chairman', 'of', '<unk>', 'general', 'electric', 'co.', 'of', 'connecticut', 'called', 'citicorp', \"'s\", 'advanced', '<unk>', 'at', '$', 'N', 'and', 'accepted', 'a', 'negative', 'hunt', 'said', 'david', '<unk>', 'president', 'of', 'wertheim', 'schroder', '&', 'co', 'later', 'nasa', 'to', 'help', 'determine', 'everyone', 'valued', 'with', 'a', 'strong', 'dollar', '<unk>', 'represented', 'its', 'stake', 'valued', 'at']\n","['plains', 'offerings', 'appeal', 'using', 'the', 'great', 'politics', 'of', 'the', 'research', 'on', 'a', '<unk>', 'va.', 'company', 'as', 'a', 'small', 'missouri', \"'s\", 'role', 'in', 'the', '<unk>', 'business', 'of', 'financing', 'could', 'be', 'so', 'modest', 'progress', 'in', 'which', 'will', 'help', 'clients', 'there', 'is', 'a', 'significant', 'or', 'bad', 'payment', 'on', 'the', 'courter', 'and', 'acknowledges', 'that']\n","['crush', 'hess', \"'s\", 'offer', 'will', 'start', 'to', 'the', 'foreign', 'exchange', \"'s\", 'decision', 'to', 'save', 'N', 'years', 'in', 'a', 'movie', 'since', 'the', 'tokyo', 'exchange', 'well', 'where', 'revenue', 'seem', 'doing', 'view', 'with', 'the', 'irs', 'spent', 'N', 'years', 'ago', 'but', 'during', 'the', 'quarter', 'is', 'expected', 'to', 'be', 'nearly', 'N', 'times', 'expanding', 'sterling', 'at']\n","['convicted', 'west', 'germany', 'arranged', '$', 'N', 'billion', 'of', '<unk>', 'control', 'board', 'of', 'corp.', 'of', 'its', 'luxury', 'infiniti', 'and', 'others', 'raw', 'materials', 'announced', 'management', 'fees', 'which', 'is', 'N', '<unk>', 'and', 'half', 'concluded', 'and', 'to', 'increase', 'overnight', 'security', 'checks', 'before', 'the', 'new', 'entity', 'squeeze', 'aspects', 'of', 'the', 'buy-back', 'of', 'the', 'vast', 'new']\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["165it [00:37,  4.38it/s]\n"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Time [0:13:47], Epoch [22/50], loss: 3.5851\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["165it [00:37,  4.36it/s]\n"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Time [0:14:25], Epoch [23/50], loss: 3.5621\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["165it [00:37,  4.37it/s]\n"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Time [0:15:03], Epoch [24/50], loss: 3.5423\n"]},{"output_type":"stream","name":"stderr","text":["165it [00:37,  4.37it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Time [0:15:41], Epoch [25/50], loss: 3.5225\n"]},{"output_type":"stream","name":"stderr","text":["165it [00:37,  4.36it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Time [0:16:18], Epoch [26/50], loss: 3.5051\n","[Generated Sentences]\n","['struggles', 'to', 'be', 'improved', 'portfolio', 'as', '$', 'N', 'million', 'for', 'its', 'lower', 'quarterly', 'sales', 'of', '$', 'N', 'million', 'or', '$', 'N', 'a', 'share', 'down', 'from', '$', 'N', 'billion', 'largely', 'because', 'qintex', 'australia', 'stated', 'its', 'corporate', 'bank', 'into', 'britain', 'but', 'to', '<unk>', 'other', 'economists', 'divide', 'into', '<unk>', 'financial', 'information', 'and', 'equipment']\n","['privileges', 'decreased', 'support', 'to', 'a', 'percentage', 'point', 'on', 'the', 'chicago', 'board', 'became', 'the', 'same', 'newspapers', 'makes', 'products', 'in', 'the', 'house', 'of', 'commons', 'shopping', 'and', 'if', 'its', 'highly', 'profitable', 'guaranty', 'corp.', 'unit', 'was', 'renamed', 'crane', 'new', 'data', 'to', 'management', 'fixed-income', 'securities', 'at', '<unk>', 'fleming', 'securities', 'ltd.', 'the', 'new', 'york', 'firm', 'of']\n","['fighter', 'patience', 'threatens', 'accumulated', 'unusual', 'centers', 'over', 'at', 'least', 'half', 'of', 'the', 'next', 'year', 'and', 'examine', 'the', 'debt', 'ceiling', 'will', 'rise', 'N', 'N', 'to', '$', 'N', 'million', 'or', 'N', 'cents', 'a', 'share', 'a', 'year', 'earlier', 'according', 'to', 'a', 'national', 'council', 'of', 'materials', 'published', 'the', 'gain', 'to', 'the', 'area', 'of', 'the']\n","['intentionally', 'photos', 'tracks', 'technology', 'issues', 'families', 'or', 'about', 'N', 'million', 'of', 'its', '<unk>', 'broadcast', 'division', 'used', 'by', '<unk>', 'financier', 'sir', 'james', 'goldsmith', \"'s\", 'first', 'joint-venture', 'announcement', 'friday', 'may', 'report', 'a', 'loss', 'of', '$', 'N', 'million', 'account', 'to', 'gulf', 'power', 'as', 'being', 'acquired', 'as', 'possible', 'to', 'keep', 'quickly', '<unk>', 'compared', 'with']\n","['assumptions', 'see', 'particular', 'trapped', 'in', 'particular', 'a', 'horse', 'from', 'asbestos', 'mortgage', 'funding', 'in', 'canada', 'under', 'the', 'enormous', 'scams', 'and', 'metal', 'pits', 'in', 'suit', 'securities', 'that', 'was', 'at', 'the', 'company', \"'s\", 'largest', 'this', 'company', 'with', 'its', 'bids', 'while', 'customers', 'try', 'to', 'withdraw', 'from', 'its', 'way', 'with', 'tourism', 'japanese', 'rules', 'making', 'credit']\n"]},{"output_type":"stream","name":"stderr","text":["165it [00:37,  4.36it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Time [0:16:56], Epoch [27/50], loss: 3.4875\n"]},{"output_type":"stream","name":"stderr","text":["165it [00:37,  4.37it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Time [0:17:34], Epoch [28/50], loss: 3.4736\n"]},{"output_type":"stream","name":"stderr","text":["165it [00:37,  4.36it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Time [0:18:12], Epoch [29/50], loss: 3.4581\n"]},{"output_type":"stream","name":"stderr","text":["165it [00:37,  4.37it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Time [0:18:50], Epoch [30/50], loss: 3.4437\n"]},{"output_type":"stream","name":"stderr","text":["165it [00:37,  4.37it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Time [0:19:27], Epoch [31/50], loss: 3.4301\n","[Generated Sentences]\n","['arnold', 'levine', 'acting', 'co-chief', 'executive', 'had', 'been', 'experiencing', 'softening', 'margins', 'on', 'controllers', 'and', 'books', 'but', 'the', 'purchasers', 'refused', 'to', 'disclose', 'their', 'operations', 'required', 'to', 'achieve', 'foreign', 'ownership', 'in', 'the', 'u.s.', 'as', 'a', 'junior', 'note', 'in', 'that', 'period', 'of', 'cars', 'currently', 'back', 'because', 'computers', '<unk>', 'existing', 'developments', 'however', 'as', 'the', 'cars']\n","['mips', 'is', 'keeping', 'in', 'europe', 'or', 'stock-index', 'futures', 'to', 'private', 'investors', 'property', 'from', 'its', 'home', 'products', 'that', 'are', 'followed', 'in', 'the', 'table', 'until', 'the', 'stock', 'price', 'movements', 'and', 'over', 'the', 'exchanges', 'in', 'the', 'tax-exempt', 'had', 'year', 'and', 'in', 'an', 'effort', 'to', 'hold', 'back', '<unk>', 'of', 'a', 'major', 'security', 'as', 'the']\n","['devoted', 'wives', 'tell', 'them', 'often', 'few', '<unk>', 'in', 'those', 'who', 'fear', 'a', 'wide', '<unk>', 'of', 'president', 'barre', \"'s\", 'home', 'and', 'scientific', 'faculty', 'the', 'report', 'says', 'both', 'men', 'if', '<unk>', 'remaining', 'cautiously', 'republicans', 'are', 'corn', 'and', 'vans', 'which', 'calls', 'for', 'racial', 'minorities', 'and', 'kept', 'his', 'york', 'within', 'a', 'few', 'weeks', 'largely']\n","['plane', 'made', 'them', 'he', 'could', 'further', '<unk>', 'known', 'as', 'he', 'says', 'if', 'you', 'can', 'support', 'this', 'and', 'that', 'only', 'after', 'the', 'church', 'of', 'these', 'days', 'many', 'people', 'would', 'need', 'success', 'with', 'funny', 'hall', 'jeff', 'residents', 'to', 'grab', 'the', 'food', 'genes', 'this', 'aim', 'as', 'fur', 'with', 'programming', 'reports', 'in', 'a', 'standing']\n","['comparisons', 'with', 'an', 'initial', 'offer', 'for', 'the', 'first', 'time', 'raise', '$', 'N', 'million', 'of', 'debt', 'limit', 'but', 'earlier', 'this', 'year', 'to', 'buy', 'knight-ridder', 'inc.', \"'s\", 'N', 'remic', 'issuance', 'to', '$', 'N', 'billion', 'is', 'both', 'more', 'than', 'N', 'N', 'for', 'six', 'years', 'with', 'N', 'N', 'in', 'composite', 'trading', 'on', 'an', '<unk>']\n"]},{"output_type":"stream","name":"stderr","text":["165it [00:37,  4.36it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Time [0:20:05], Epoch [32/50], loss: 3.4180\n"]},{"output_type":"stream","name":"stderr","text":["165it [00:37,  4.36it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Time [0:20:43], Epoch [33/50], loss: 3.4082\n"]},{"output_type":"stream","name":"stderr","text":["165it [00:37,  4.38it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Time [0:21:21], Epoch [34/50], loss: 3.3984\n"]},{"output_type":"stream","name":"stderr","text":["165it [00:37,  4.36it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Time [0:21:59], Epoch [35/50], loss: 3.3878\n"]},{"output_type":"stream","name":"stderr","text":["165it [00:37,  4.36it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Time [0:22:37], Epoch [36/50], loss: 3.3785\n","[Generated Sentences]\n","['dealing', 'paper', 'losses', 'losing', 'money', 'on', 'the', 'price', 'movements', 'mostly', 'costs', '<unk>', 'positions', 'when', 'to', 'cover', 'all', 'insurance', 'issues', 'such', 'as', 'sales', 'at', 'gulf', 'power', 'especially', 'despite', 'the', '<unk>', 'for', 'the', 'first', 'time', 'and', 'is', 'designed', 'to', 'acquire', 'the', 'strengthening', 'disk', 'drive', 'and', 'allow', 'the', 'federal', 'reserve', 'activity', 'has', 'sharply']\n","['outlays', 'for', 'the', 'yield', 'on', 'asset-backed', 'certificates', 'of', 'bonds', 'due', 'march', 'N', 'cents', 'a', 'bottle', 'yen', 'he', 'said', 'its', 'changes', 'were', 'made', 'in', 'big', 'terms', 'and', 'the', 'proposed', 'penalties', 'would', 'bring', 'in', 'the', 'purchase', 'because', 'of', 'expectations', 'had', 'been', 'sophisticated', 'down', 'to', '<unk>', 'companies', 'for', 'N', 'employees', 'and', 'leaving', 'the']\n","['lung', 'achievement', 'expense', 'is', 'also', '<unk>', 'or', 'trying', 'to', 'build', 'a', '$', 'N', 'million', 'to', '$', 'N', 'million', 'aid', 'plan', 'for', 'one', 'year', 'of', 'falling', 'rates', 'on', 'speculation', 'that', 'the', 'expected', 'repeal', 'of', 'order', 'will', 'take', 'place', 'at', 'the', 'close', '<unk>', 'the', 'selling', 'of', 'u.s.', 'policy', 'possible', 'buying', 'during', 'the']\n","['ten', '<unk>', 'ringers', 'pay', 'jim', '<unk>', 'nonetheless', 'issue', 'market', 'because', 'they', 'merely', 'an', 'eagerness', 'to', 'kill', 'advantage', 'of', 'future', 'colleagues', 'in', 'a', 'luxury', 'car', 'council', 'delegate', 'authority', '<unk>', 'food', 'test', 'last', 'spring', 'season', 'to', 'fraud', 'offers', 'for', 'as', 'little', 'as', 'age', 'and', 'the', 'developer', '<unk>', 'of', 'state', 'and', 'robert', 'gates']\n","['lights', 'british', 'air', 'france', 'may', 'set', 'up', 'in', 'early', 'september', 'step', 'on', 'order', 'to', 'use', 'liquidity', '<unk>', 'down', 'with', 'account', 'in', 'public', 'hands', 'of', 'N', 'N', 'in', '<unk>', 'data', 'eased', 'to', 'N', 'yen', 'up', 'from', 'N', 'N', 'including', 'N', 'million', 'guilders', 'or', '$', 'N', 'a', 'share', 'up', 'from', 'a', 'year']\n"]},{"output_type":"stream","name":"stderr","text":["165it [00:37,  4.37it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Time [0:23:15], Epoch [37/50], loss: 3.3707\n"]},{"output_type":"stream","name":"stderr","text":["165it [00:37,  4.36it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Time [0:23:52], Epoch [38/50], loss: 3.3632\n"]},{"output_type":"stream","name":"stderr","text":["165it [00:37,  4.38it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Time [0:24:30], Epoch [39/50], loss: 3.3533\n"]},{"output_type":"stream","name":"stderr","text":["165it [00:37,  4.37it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Time [0:25:08], Epoch [40/50], loss: 3.3456\n"]},{"output_type":"stream","name":"stderr","text":["165it [00:37,  4.36it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Time [0:25:46], Epoch [41/50], loss: 3.3407\n","[Generated Sentences]\n","['diet', 'coke', 'will', 'become', 'president', 'of', 'labor', 'by', 'george', 'd.', '<unk>', 'in', 'the', 'self-employed', 'as', 'chief', 'executive', 'officer', 'of', 'mcgraw-hill', 'inc.', 'minneapolis', 'and', 'electronics', '<unk>', 'when', 'trans', 'world', 'airlines', 'spent', 'N', 'cents', 'and', '$', 'N', 'to', '$', 'N', 'up', 'from', '$', 'N', 'late', 'monday', 'monday', 'in', 'california', 'but', 'it', 'a']\n","['dating', 'wages', 'inside', 'the', 'galileo', 'worm', 'had', 'cut', 'the', 'risks', 'on', 'such', 'businesses', 'although', 'the', 'expected', 'increase', 'will', 'include', 'a', 'N', '<unk>', 'coupon', 'division', 'of', 'capital', 'which', 'holds', '$', 'N', 'million', 'has', 'suspended', 'its', 'public', 'stock', 'specialist', 'company', 'said', 'it', 'expects', 'sales', 'to', 'accelerate', 'delivery', 'showed', 'through', 'petco', 'in', 'allied-signal']\n","['underwriters', 'specifically', 'generated', 'fha', 'such', 'as', 'solid', 'shoes', 'and', 'running', 'strategic', 'operations', 'and', 'the', 'investment', 'banking', 'says', 'even', 'investors', 'were', 'because', 'expansion', 'of', 'sci', 'tv', \"'s\", 'group', 'one', 'of', 'the', 'market', 'based', 'on', 'crop', 'business', 'and', 'stock', 'which', 'should', 'remain', 'stable', 'and', 'must', 'meet', 'real', 'until', 'they', 'had', 'such', 'trade']\n","['punishment', 'holders', 'assumed', 'that', 'it', 'is', \"n't\", 'any', 'decision', 'many', 'mailing', 'on', 'any', 'of', 'recent', 'stocks', 'and', 'reduced', 'profit', 'margins', 'of', 'shareholder', 'equity', 'for', 'september', 'a', '<unk>', 'holding', 'company', 'a', 'belgian', 'company', 'based', 'in', 'hartford', 'which', 'rose', 'N', 'N', 'to', 'N', 'million', 'on', 'N', 'million', 'shares', 'canadian', 'dollars', 'that', 'few']\n","['commonly', 'funding', 'favored', 'development', 'corp.', 'and', 'japan', \"'s\", 'media', 'funds', 'cost', 'to', 'the', 'public', 'housing', 'sector', 'of', 'singapore', 'property', 'interests', 'were', 'purchased', 'by', '<unk>', 'programs', 'in', 'the', 'california', 'firm', 'issued', 'confusion', 'from', 'the', 'combined', '<unk>', 'city', 'food', 'company', \"'s\", 'common', 'and', 'employee', 'stock', 'analyst', 'at', 'montgomery', 'securities', 'totaled', '$', 'N']\n"]},{"output_type":"stream","name":"stderr","text":["165it [00:37,  4.37it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Time [0:26:24], Epoch [42/50], loss: 3.3371\n"]},{"output_type":"stream","name":"stderr","text":["165it [00:37,  4.36it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Time [0:27:01], Epoch [43/50], loss: 3.3278\n"]},{"output_type":"stream","name":"stderr","text":["165it [00:37,  4.37it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Time [0:27:39], Epoch [44/50], loss: 3.3241\n"]},{"output_type":"stream","name":"stderr","text":["165it [00:37,  4.37it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Time [0:28:17], Epoch [45/50], loss: 3.3178\n"]},{"output_type":"stream","name":"stderr","text":["165it [00:37,  4.36it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Time [0:28:55], Epoch [46/50], loss: 3.3118\n","[Generated Sentences]\n","['f.', '<unk>', 'announced', '$', 'N', 'million', 'on', 'profit', 'per', 'third', 'quarter', 'soared', 'in', 'the', 'fact', 'that', 'it', 'would', 'create', 'N', 'companies', 'all', 'along', 'with', 'certain', 'imports', 'the', 'toronto', 'stock', 'to', 'compiled', 'by', 'military', 'loans', 'say', 'they', 'are', \"n't\", 'connected', 'with', 'the', 'local', 'damage', 'but', 'perhaps', 'less', '<unk>', 'of', 'selling', 'dominated']\n","['lost', 'cholesterol', 'for', 'traffic', 'behind', '<unk>', '<unk>', 'no', 'ethical', 'resistance', 'after', 'the', 'announcement', '<unk>', 'that', 'their', 'substitute', 'values', 'the', 'responsibility', 'in', 'his', 'huge', '<unk>', 'environment', 'which', 'is', 'primarily', 'and', 'totally', 'the', 'right', 'time', 'magazine', \"'s\", 'first', 'the', 'ocean', 'lasting', 'warning', 'to', 'another', '<unk>', 'audit', 'that', 'which', 'one', '<unk>', 'seen', 'closed']\n","['fidelity', 'treasurys', 'will', 'now', 'be', 'completed', 'by', 'subsidizing', 'acquisition', 'in', 'N', 'to', 'N', 'N', 'from', 'N', '<unk>', 'a', 'year', 'ago', 'in', 'september', 'including', 'macmillan', 'lots', 'of', 'the', 'conservative', '<unk>', 'portfolios', 'used', 'in', 'countries', 'and', 'the', 'legal', 'battle', 'and', 'encouraged', 'by', 'an', 'increase', 'of', '#', 'N', 'billion', 'dollars', 'to', 'floors', 'and']\n","['suez', 'a', 'san', 'francisco', 'lawyer', 'says', 'that', '<unk>', 'skin', '<unk>', 'them', 'mr.', 'andersson', 'and', 'the', 'dallas', 'partner', 'industry', 'declined', 'to', 'elaborate', 'on', 'the', 'issue', 'stock-index', 'futures', 'markets', 'for', 'this', 'theory', 'to', 'a', 'majority', 'of', 'traders', 'who', 'says', 'now', 'it', 'is', 'difficult', 'to', 'tell', 'the', 'cost', 'toward', 'growing', 'big', 'times', 'every']\n","['practice', 'artery', 'challenges', 'advocates', 'jones', 'don', '<unk>', 'station', 'ltd', 'nobel', 'science', 'said', 'gary', 'shapiro', 'vice', 'president', 'for', '<unk>', 'paris', 'was', '<unk>', 'into', 'an', '<unk>', 'shop', 'where', 'unscrupulous', 'lawyers', 'had', 'an', 'operating', 'officer', 'of', '<unk>', \"'s\", '<unk>', 'park', '<unk>', 'a', 'week', 'of', 'new', 'york', 'president', 'christopher', 'dodd', \"'s\", 'senior', 'vice', 'president']\n"]},{"output_type":"stream","name":"stderr","text":["165it [00:37,  4.37it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Time [0:29:33], Epoch [47/50], loss: 3.3100\n"]},{"output_type":"stream","name":"stderr","text":["165it [00:37,  4.37it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Time [0:30:10], Epoch [48/50], loss: 3.3035\n"]},{"output_type":"stream","name":"stderr","text":["165it [00:37,  4.37it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Time [0:30:48], Epoch [49/50], loss: 3.3010\n"]},{"output_type":"stream","name":"stderr","text":["165it [00:37,  4.38it/s]"]},{"output_type":"stream","name":"stdout","text":["Time [0:31:26], Epoch [50/50], loss: 3.2972\n"]},{"output_type":"stream","name":"stderr","text":["\n"]}]},{"cell_type":"markdown","metadata":{"id":"nDhlrcENM4Dx"},"source":["생성된 텍스트의 퀄리티는 어떤가요? \n","\n","앞으로 딥러닝 강의가 끝나면 자연어처리 강좌에서 텍스트 처리에 적합한 전처리 과정, 모델구조들을 본격적으로 배우시게 될것입니다."]},{"cell_type":"markdown","metadata":{"id":"1Ua-_6W2a5Lt"},"source":["# References\n","\n","1. https://github.com/pytorch/examples/tree/master/word_language_model\n","2. https://github.com/yunjey/pytorch-tutorial/tree/master/tutorials/02-intermediate/language_model"]}]}